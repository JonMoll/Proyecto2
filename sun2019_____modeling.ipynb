{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sun2019_____modeling.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRF9LH6iJ5fF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import copy\n",
        "import json\n",
        "import math\n",
        "\n",
        "import six\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import CrossEntropyLoss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVWLvvwDc_Y7",
        "colab_type": "text"
      },
      "source": [
        "# 1. GELU\n",
        "\n",
        "* x:\n",
        "\n",
        "* torch.erf: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9WXga1PVYgT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gelu(x):\n",
        "    \"\"\"Implementation of the gelu activation function.\n",
        "        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
        "        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "    \"\"\"\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GPcGYEHdCiG",
        "colab_type": "text"
      },
      "source": [
        "# 2. BERT Config\n",
        "\n",
        "Guarda la configuracion del modelo BERT\n",
        "\n",
        "* vocab_size: Tamaño del vocabulario de inputs_ids\n",
        "* hidden_size: Tamaño de las capas del codificador y la capa de pool\n",
        "* num_hidden_layers: Número de capas ocultas en el codificador de Transformer\n",
        "* num_attention_heads: Número de attention heads para cada capa de atención en el codificador Transformer\n",
        "* intermediate_size: Tamaño de la capa intermedia en el codificador de Transformer\n",
        "* hidden_act: Función de activación no lineal del codificador y el pool\n",
        "* hidden_dropout_prob: Probabilidad de dropout de todas las capas completamente conectadas en los embeddings, el codificador y el pool\n",
        "* attention_probs_dropout_prob: Probabilidad de dropout para las probabilidades de atencion\n",
        "* max_position_embeddings: Longitud de la secuencia máxima que se puede introducir al modelo\n",
        "* type_vocab_size: Tamaño del vocabulario de token_type_ids\n",
        "* initializer_range:  Valor de sttdev para truncated_normal_initializer que inicializa todas las matrices de pesos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1hXLJBFVl1Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertConfig(object):\n",
        "    def __init__(self,\n",
        "                vocab_size,\n",
        "                hidden_size=768,\n",
        "                num_hidden_layers=12,\n",
        "                num_attention_heads=12,\n",
        "                intermediate_size=3072,\n",
        "                hidden_act=\"gelu\",\n",
        "                hidden_dropout_prob=0.1,\n",
        "                attention_probs_dropout_prob=0.1,\n",
        "                max_position_embeddings=512,\n",
        "                type_vocab_size=16,\n",
        "                initializer_range=0.02):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.hidden_act = hidden_act\n",
        "        self.intermediate_size = intermediate_size\n",
        "        self.hidden_dropout_prob = hidden_dropout_prob\n",
        "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
        "        self.max_position_embeddings = max_position_embeddings\n",
        "        self.type_vocab_size = type_vocab_size\n",
        "        self.initializer_range = initializer_range"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN9Djuhqi-9R",
        "colab_type": "text"
      },
      "source": [
        "# 2.1. From dict\n",
        "\n",
        "Construye un objeto BertConfig a partir de un diccionario de Python\n",
        "\n",
        "* json_object: Objeto con la informacion en un diccionario de un archivo json del dataset\n",
        "\n",
        "* config: Objeto BertConfig creado a partir de un diccionario\n",
        "* six.iteritems: Compatibilidad para recorrer diccionarios de Python 2 y 3\n",
        "* `config.__dict__[key]`: En el diccionario por defecto del objeto config se crea una entrada con el par (key, value) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuOCuP40VzpB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    @classmethod\n",
        "    def from_dict(cls, json_object):\n",
        "        config = BertConfig(vocab_size=None)\n",
        "        for (key, value) in six.iteritems(json_object):\n",
        "            config.__dict__[key] = value\n",
        "        return config"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dz5_ORvEjDtF",
        "colab_type": "text"
      },
      "source": [
        "# 2.2. From json file\n",
        "\n",
        "Construye un objeto BertConfig a partir de un archivo json\n",
        "\n",
        "* json_file: Path del archivo json\n",
        "\n",
        "* text: El texto que el archvo json contiene\n",
        "* json.loads: Transforma text en un objeto json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmTqbZ_WV8XP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    @classmethod\n",
        "    def from_json_file(cls, json_file):\n",
        "        with open(json_file, \"r\") as reader:\n",
        "            text = reader.read()\n",
        "        return cls.from_dict(json.loads(text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9T87v4V3jIMJ",
        "colab_type": "text"
      },
      "source": [
        "# 2.3. To dict\n",
        "\n",
        "Exporta el diccionario por defecto de un objeto BertConfig\n",
        "\n",
        "* copy.deepcopy: Crea una copia que incluye los elementos anidados\n",
        "* output: El diccionario por defecto de un objeto BertConfig"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFW4peglWFeR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def to_dict(self):\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6wNcbvPje5v",
        "colab_type": "text"
      },
      "source": [
        "# 2.4. To json string\n",
        "\n",
        "Exporta el diccionario por defecto de un objeto BertConfig a un string con el formato de un archivo json\n",
        "\n",
        "* json.dumps: Transforma un diccionario al formato de un archivo json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LO6QxaqWOGS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def to_json_string(self):\n",
        "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQj6ynH2jiL3",
        "colab_type": "text"
      },
      "source": [
        "# 3. BERT layer norm\n",
        "\n",
        "Capa de normalizacion al estilo de TF<br>\n",
        "La normalizacion ajusta valores en diferentes escalas respecto a una escala comun<br>\n",
        "La normalizacion de capas acelera el proceso de entrenamiento (Ba et al.)\n",
        "\n",
        "* variance_epsilon: Valor que ajusta la varianza para calcular la desviacion estandar\n",
        "* gamma: Vector de valores para ajustar la norma\n",
        "* beta: Vector de valores para ajustar la norma\n",
        "\n",
        "* nn.Module: Modulos de pytorch con operaciones basicas de redes neuronales\n",
        "* config: Objeto con los parametros de configuracion del modelo BERT\n",
        "* hidden_size: Tamaño de las capas del codificador y la capa de pool\n",
        "* torch.ones: Crea un vector de unos\n",
        "* torch.zeros: Crea un vector de ceros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_pG6GJxWtM6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BERTLayerNorm(nn.Module):\n",
        "    def __init__(self, config, variance_epsilon=1e-12):\n",
        "        super(BERTLayerNorm, self).__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(config.hidden_size))\n",
        "        self.beta = nn.Parameter(torch.zeros(config.hidden_size))\n",
        "        self.variance_epsilon = variance_epsilon"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNNGQB31jm5D",
        "colab_type": "text"
      },
      "source": [
        "# 3.1. Forward\n",
        "\n",
        "# $ Norm(x) = \\frac{x - Mean}{Standard Deviation} $\n",
        "# $ Standard Deviation = \\sqrt{Variance} $\n",
        "\n",
        "<br>\n",
        "\n",
        "La normalizacion ajusta valores en diferentes escalas respecto a una escala comun<br>\n",
        "La normalizacion de capas acelera el proceso de entrenamiento (Ba et al.)\n",
        "\n",
        "* variance_epsilon: Valor que ajusta la varianza para calcular la desviacion estandar\n",
        "* gamma: Vector de valores para ajustar la norma\n",
        "* beta: Vector de valores para ajustar la norma\n",
        "\n",
        "https://stackoverflow.com/questions/50973995/layer-normalization-and-how-it-works-tensorflow/52205057\n",
        "\n",
        "# $ Norm(x) * gamma + beta $\n",
        "Se multiplica cada dimension de $Norm(x)$ con $gamma$ y luego se le suma $beta$.<br>\n",
        "$gamma=1$ y $beta=0$ para ignorar este paso.\n",
        "\n",
        "<br>\n",
        "\n",
        "* x: Valores de entrada\n",
        "* u: Media de x\n",
        "* s: Varianza de x"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqBDbR5qW0wI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def forward(self, x):\n",
        "        u = x.mean(-1, keepdim=True)\n",
        "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
        "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
        "        return self.gamma * x + self.beta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWPSJyEydPGV",
        "colab_type": "text"
      },
      "source": [
        "# 4. BERT Embeddings\n",
        "\n",
        "* config: Objeto con los parametros de configuracion del modelo BERT\n",
        "* vocab_size: Tamaño del vocabulario de inputs_ids\n",
        "* max_position_embeddings: Longitud de la secuencia máxima que se puede introducir al modelo\n",
        "* type_vocab_size: Tamaño del vocabulario de token_type_ids\n",
        "* hidden_size: Tamaño de las capas del codificador y la capa de pool\n",
        "\n",
        "\n",
        "* self.word_embeddings: Variable para los embeddings del vocabulario de embeddings\n",
        "* self.position_embeddings: Variable para los embeddings de la secuencia de entrada\n",
        "* self.token_type_embeddings: Variable para guardar el vector que contiene el tipo del token\n",
        "\n",
        "\n",
        "* self.LayerNorm: Se le añade una capa de normalizacion (Seccion 3)\n",
        "* self.dropout: Dropout de la capa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pccCr9LXW79x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BERTEmbeddings(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BERTEmbeddings, self).__init__()\n",
        "        \"\"\"Construct the embedding module from word, position and token_type embeddings.\n",
        "        \"\"\"\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size) # nn.Embedding(vocab_size, vector_size)\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
        "\n",
        "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
        "        # any TensorFlow checkpoint file\n",
        "        self.LayerNorm = BERTLayerNorm(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gq4JJz5lrTz9",
        "colab_type": "text"
      },
      "source": [
        "# 4.1. Forward\n",
        "\n",
        "<img src=https://cdn-images-1.medium.com/max/1200/0*m_kXt3uqZH9e7H4w.png>\n",
        "\n",
        "**Token Embedding**\n",
        "\n",
        "```\n",
        "input_ids = torch.LongTensor([[31, 51, 99], \n",
        "                              [15,  5,  0]])\n",
        "```\n",
        "**Positional Embedding**\n",
        "```\n",
        "seq_length = 3\n",
        "position_ids (torch.arange): tensor([0, 1, 2])\n",
        "position_ids (unsqueeze): tensor([[0, 1, 2],\n",
        "                                  [0, 1, 2]])\n",
        "```\n",
        "**Sentence Embedding**\n",
        "```\n",
        "token_type_ids (torch.zeros_like): tensor([[0, 0, 0],\n",
        "                                           [0, 0, 0]])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivp2Ij0TXES7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def forward(self, input_ids, token_type_ids=None):\n",
        "        seq_length = input_ids.size(1)\n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
        "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros_like(input_ids)\n",
        "\n",
        "        words_embeddings = self.word_embeddings(input_ids)\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "\n",
        "        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqFoKYeWdU6a",
        "colab_type": "text"
      },
      "source": [
        "# 5. BERT Self Attention\n",
        "\n",
        "* config: Objeto con los parametros de configuracion del modelo BERT\n",
        "\n",
        "\n",
        "* self.num_attention_heads: Número de attention heads para cada capa de atención en el codificador \n",
        "Transformer\n",
        "\n",
        "```\n",
        "num_attention_heads = 12\n",
        "hidden_size = 768\n",
        "attention_head_size = 768/12 = 64\n",
        "all_head_size = 12*64 = 768\n",
        "```\n",
        "\n",
        "* nn.Linear: Aplica una transformacion lineal a la entrada\n",
        "\n",
        "# $ Y =X * W^{T} + bias $\n",
        "\n",
        "\n",
        "```\n",
        "m = nn.Linear(2, 1) # 2 nodos (entrada) y 1 nodo (salida)\n",
        "m.weight: tensor([[0.2683, 0.2599]])\n",
        "m.bias: tensor([0.6741])\n",
        "\n",
        "input = torch.tensor([[1.0, -1.0]]) \n",
        "\n",
        "output = m(input)\n",
        "1.0 * 0.2683 - 1.0 * 0.2599 + 0.6741 = 0.6825\n",
        "output: tensor([[0.6825]]\n",
        "\n",
        "```\n",
        "\n",
        "* self.query: 768 nodos (entrada) y 768 nodos (salida)\n",
        "* self.key:     768 nodos (entrada) y 768 nodos (salida)\n",
        "* self.value:  768 nodos (entrada) y 768 nodos (salida)\n",
        "\n",
        "\n",
        "* self.dropout: Dropout de la capa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iorsW4BSXLGy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BERTSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BERTSelfAttention, self).__init__()\n",
        "        if config.hidden_size % config.num_attention_heads != 0:\n",
        "            raise ValueError(\n",
        "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads))\n",
        "        \n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key =   nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9wDfDuSrXIt",
        "colab_type": "text"
      },
      "source": [
        "# 5.1. Transpose for scores\n",
        "\n",
        "* x: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq6Wgj4aXUeq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHKnloY5r1jd",
        "colab_type": "text"
      },
      "source": [
        "# 5.2. Forward\n",
        "\n",
        "* hidden_states: Entrada de las capas de transformacion lineal, vector de tamaño 768\n",
        "* attention_mask: \n",
        "\n",
        "* mixed_query_layer: Salida de las capas de transformacion lineal, vector de tamaño 768\n",
        "* mixed_key_layer: Salida de las capas de transformacion lineal, vector de tamaño 768\n",
        "* mixed_value_layer: Salida de las capas de transformacion lineal, vector de tamaño 768\n",
        "\n",
        "* query_layer: \n",
        "* key_layer: \n",
        "* value_layer: \n",
        "* attention_scores: \n",
        "* attention_probs: \n",
        "* context_layer: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4lN5vtmXavD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def forward(self, hidden_states, attention_mask):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        mixed_key_layer = self.key(hidden_states)\n",
        "        mixed_value_layer = self.value(hidden_states)\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
        "        attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "        return context_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMnddQAidgTQ",
        "colab_type": "text"
      },
      "source": [
        "# 6. BERT Self Output\n",
        "\n",
        "* config: \n",
        "\n",
        "* self.dense: \n",
        "* self.LayerNorm: \n",
        "* self.dropout: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkAXYVsIXh2z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BERTSelfOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BERTSelfOutput, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.LayerNorm = BERTLayerNorm(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1pBvzvVr8n9",
        "colab_type": "text"
      },
      "source": [
        "# 6.1. Forward\n",
        "\n",
        "* hidden_states: \n",
        "* input_tensor: \n",
        "\n",
        "* self.dense: \n",
        "* self.dropout: \n",
        "* self.LayerNorm: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5wmi5m8XtNK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pCmnOt8dr0H",
        "colab_type": "text"
      },
      "source": [
        "# 7. BERT Attention\n",
        "\n",
        "* config: \n",
        "\n",
        "* self.output: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqjwF4c5Xopx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BERTAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BERTAttention, self).__init__()\n",
        "        self.self = BERTSelfAttention(config)\n",
        "        self.output = BERTSelfOutput(config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxBcCdcDsE3P",
        "colab_type": "text"
      },
      "source": [
        "# 7.1. Forward\n",
        "\n",
        "* input_tensor: \n",
        "* attention_mask: \n",
        "\n",
        "* attention_output: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27E0OvQvXyo_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def forward(self, input_tensor, attention_mask):\n",
        "        self_output = self.self(input_tensor, attention_mask)\n",
        "        attention_output = self.output(self_output, input_tensor)\n",
        "        return attention_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy83DCg9dwKd",
        "colab_type": "text"
      },
      "source": [
        "# 8. BERT Intermediate\n",
        "\n",
        "* config: \n",
        "\n",
        "* self.dense: \n",
        "* self.intermediate_act_fn: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OkLKA7qX5ih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BERTIntermediate(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BERTIntermediate, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        self.intermediate_act_fn = gelu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLgS86FVsQXX",
        "colab_type": "text"
      },
      "source": [
        "# 8.1. Forward\n",
        "\n",
        "* hidden_states: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IldmAeTAX-uB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "        return hidden_states"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9p7Sh-Od3Ww",
        "colab_type": "text"
      },
      "source": [
        "# 9. BERT Output\n",
        "\n",
        "* config: \n",
        "\n",
        "* self.dense: \n",
        "* self.LayerNorm: \n",
        "* self.dropout: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2K--kvEYDoa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BERTOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BERTOutput, self).__init__()\n",
        "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.LayerNorm = BERTLayerNorm(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpfTPSbzssFt",
        "colab_type": "text"
      },
      "source": [
        "# 9.1. Forward\n",
        "\n",
        "* hidden_states: \n",
        "* input_tensor: \n",
        "\n",
        "* self.dense: \n",
        "* self.dropout: \n",
        "* self.LayerNorm: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiAK7WgNYMYb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRyhJs--d94y",
        "colab_type": "text"
      },
      "source": [
        "# 10. BERT Layer\n",
        "\n",
        "* config: \n",
        "\n",
        "* self.attention: \n",
        "* self.intermediate: \n",
        "* self.output: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tozMOEQLYWjh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BERTLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BERTLayer, self).__init__()\n",
        "        self.attention = BERTAttention(config)\n",
        "        self.intermediate = BERTIntermediate(config)\n",
        "        self.output = BERTOutput(config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPGL3WCus4dm",
        "colab_type": "text"
      },
      "source": [
        "# 10.1. Forward\n",
        "\n",
        "* hidden_states: \n",
        "* attention_mask: \n",
        "\n",
        "* attention_output: \n",
        "* intermediate_output: \n",
        "* layer_output: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkHCV-0_Ycfh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def forward(self, hidden_states, attention_mask):\n",
        "        attention_output = self.attention(hidden_states, attention_mask)\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        layer_output = self.output(intermediate_output, attention_output)\n",
        "        return layer_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDWcs8AIeIfM",
        "colab_type": "text"
      },
      "source": [
        "# 11. BERT Encoder\n",
        "\n",
        "* config: \n",
        "\n",
        "* layer: \n",
        "* nn.ModuleList: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iX3JEWIQYhyD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BERTEncoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BERTEncoder, self).__init__()\n",
        "        layer = BERTLayer(config)\n",
        "        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(config.num_hidden_layers)])    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggXdb0Sps6eR",
        "colab_type": "text"
      },
      "source": [
        "# 11.1. Forward\n",
        "\n",
        "* hidden_states: \n",
        "* attention_mask: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdLMVQlOYmiF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def forward(self, hidden_states, attention_mask):\n",
        "        all_encoder_layers = []\n",
        "        for layer_module in self.layer:\n",
        "            hidden_states = layer_module(hidden_states, attention_mask)\n",
        "            all_encoder_layers.append(hidden_states)\n",
        "        return all_encoder_layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4N-LTRkeLVj",
        "colab_type": "text"
      },
      "source": [
        "# 12. BERT Pooler\n",
        "\n",
        "* config: \n",
        "\n",
        "* self.dense: \n",
        "* self.activation: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dz--ojaDYrTU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BERTPooler(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BERTPooler, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.activation = nn.Tanh()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnvWZG6Gsy6x",
        "colab_type": "text"
      },
      "source": [
        "# 12.1. Forward\n",
        "\n",
        "* hidden_states: \n",
        "\n",
        "* first_token_tensor: \n",
        "* pooled_output: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbCbK_rvYwY1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def forward(self, hidden_states):\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "        # to the first token.\n",
        "        first_token_tensor = hidden_states[:, 0]\n",
        "        #return first_token_tensor\n",
        "        pooled_output = self.dense(first_token_tensor)\n",
        "        pooled_output = self.activation(pooled_output)\n",
        "        return pooled_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym4V5U45eRu8",
        "colab_type": "text"
      },
      "source": [
        "# 13. BERT Model\n",
        "\n",
        "* config: \n",
        "\n",
        "* self.embeddings: \n",
        "* self.encoder: \n",
        "* self.pooler: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rIV_TVoeS8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertModel(nn.Module):\n",
        "    \"\"\"BERT model (\"Bidirectional Embedding Representations from a Transformer\").\n",
        "\n",
        "    Example usage:\n",
        "    ```python\n",
        "    # Already been converted into WordPiece token ids\n",
        "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
        "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
        "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 2, 0]])\n",
        "\n",
        "    config = modeling.BertConfig(vocab_size=32000, hidden_size=512,\n",
        "        num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n",
        "\n",
        "    model = modeling.BertModel(config=config)\n",
        "    all_encoder_layers, pooled_output = model(input_ids, token_type_ids, input_mask)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self, config: BertConfig):\n",
        "        \"\"\"Constructor for BertModel.\n",
        "\n",
        "        Args:\n",
        "            config: `BertConfig` instance.\n",
        "        \"\"\"\n",
        "        super(BertModel, self).__init__()\n",
        "        self.embeddings = BERTEmbeddings(config)\n",
        "        self.encoder = BERTEncoder(config)\n",
        "        self.pooler = BERTPooler(config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3h35b3Bs9U3",
        "colab_type": "text"
      },
      "source": [
        "# 13.1. Forward\n",
        "\n",
        "* input_ids: \n",
        "* token_type_ids: \n",
        "* attention_mask: \n",
        "\n",
        "* extended_attention_mask: \n",
        "* embedding_output: \n",
        "* all_encoder_layers: \n",
        "* sequence_output: \n",
        "* pooled_output: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7o0E0taY0lE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones_like(input_ids)\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros_like(input_ids)\n",
        "\n",
        "        # We create a 3D attention mask from a 2D tensor mask.\n",
        "        # Sizes are [batch_size, 1, 1, from_seq_length]\n",
        "        # So we can broadcast to [batch_size, num_heads, to_seq_length, from_seq_length]\n",
        "        # this attention mask is more simple than the triangular masking of causal attention\n",
        "        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
        "        # masked positions, this operation will create a tensor which is 0.0 for\n",
        "        # positions we want to attend and -10000.0 for masked positions.\n",
        "        # Since we are adding it to the raw scores before the softmax, this is\n",
        "        # effectively the same as removing these entirely.\n",
        "        extended_attention_mask = extended_attention_mask.float()\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "        embedding_output = self.embeddings(input_ids, token_type_ids)\n",
        "        all_encoder_layers = self.encoder(embedding_output, extended_attention_mask)\n",
        "        sequence_output = all_encoder_layers[-1]\n",
        "        pooled_output = self.pooler(sequence_output)\n",
        "        return all_encoder_layers, pooled_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8H_Ux4ZebUY",
        "colab_type": "text"
      },
      "source": [
        "# 14. BERT for Sequence Classification\n",
        "\n",
        "* config: \n",
        "* num_labels: \n",
        "\n",
        "* self.bert: \n",
        "* self.dropout: \n",
        "* self.classifier: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aF53KGqUY44t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertForSequenceClassification(nn.Module):\n",
        "    \"\"\"BERT model for classification.\n",
        "    This module is composed of the BERT model with a linear layer on top of\n",
        "    the pooled output.\n",
        "\n",
        "    Example usage:\n",
        "    ```python\n",
        "    # Already been converted into WordPiece token ids\n",
        "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
        "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
        "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 2, 0]])\n",
        "\n",
        "    config = BertConfig(vocab_size=32000, hidden_size=512,\n",
        "        num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n",
        "\n",
        "    num_labels = 2\n",
        "\n",
        "    model = BertForSequenceClassification(config, num_labels)\n",
        "    logits = model(input_ids, token_type_ids, input_mask)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self, config, num_labels):\n",
        "        super(BertForSequenceClassification, self).__init__()\n",
        "        self.bert = BertModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
        "\n",
        "        def init_weights(module):\n",
        "            if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "                # Slightly different from the TF version which uses truncated_normal for initialization\n",
        "                # cf https://github.com/pytorch/pytorch/pull/5617\n",
        "                module.weight.data.normal_(mean=0.0, std=config.initializer_range)\n",
        "            elif isinstance(module, BERTLayerNorm):\n",
        "                module.beta.data.normal_(mean=0.0, std=config.initializer_range)\n",
        "                module.gamma.data.normal_(mean=0.0, std=config.initializer_range)\n",
        "            if isinstance(module, nn.Linear):\n",
        "                module.bias.data.zero_()\n",
        "        self.apply(init_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7S5u_hAuGBx",
        "colab_type": "text"
      },
      "source": [
        "# 14.1. Forward\n",
        "\n",
        "* input_ids: \n",
        "* token_type_ids: \n",
        "* attention_mask: \n",
        "* labels: \n",
        "\n",
        "* pooled_output: \n",
        "* logits: \n",
        "* loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_loMdeRUY9Gy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def forward(self, input_ids, token_type_ids, attention_mask, labels=None):\n",
        "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(logits, labels)\n",
        "            return loss, logits\n",
        "        else:\n",
        "            return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnMUZvYdesmi",
        "colab_type": "text"
      },
      "source": [
        "# 15. BERT for Question Answering\n",
        "\n",
        "* config: \n",
        "\n",
        "* self.bert: \n",
        "* self.qa_outputs: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6s6QbmYkZAii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertForQuestionAnswering(nn.Module):\n",
        "    \"\"\"BERT model for Question Answering (span extraction).\n",
        "    This module is composed of the BERT model with a linear layer on top of\n",
        "    the sequence output that computes start_logits and end_logits\n",
        "\n",
        "    Example usage:\n",
        "    ```python\n",
        "    # Already been converted into WordPiece token ids\n",
        "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
        "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
        "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 2, 0]])\n",
        "\n",
        "    config = BertConfig(vocab_size=32000, hidden_size=512,\n",
        "        num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n",
        "\n",
        "    model = BertForQuestionAnswering(config)\n",
        "    start_logits, end_logits = model(input_ids, token_type_ids, input_mask)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertForQuestionAnswering, self).__init__()\n",
        "        self.bert = BertModel(config)\n",
        "        # TODO check with Google if it's normal there is no dropout on the token classifier of SQuAD in the TF version\n",
        "        # self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n",
        "\n",
        "        def init_weights(module):\n",
        "            if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "                # Slightly different from the TF version which uses truncated_normal for initialization\n",
        "                # cf https://github.com/pytorch/pytorch/pull/5617\n",
        "                module.weight.data.normal_(mean=0.0, std=config.initializer_range)\n",
        "            elif isinstance(module, BERTLayerNorm):\n",
        "                module.beta.data.normal_(mean=0.0, std=config.initializer_range)\n",
        "                module.gamma.data.normal_(mean=0.0, std=config.initializer_range)\n",
        "            if isinstance(module, nn.Linear):\n",
        "                module.bias.data.zero_()\n",
        "        self.apply(init_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayca4RGruJrT",
        "colab_type": "text"
      },
      "source": [
        "# 15.1. Forward\n",
        "\n",
        "* input_ids: \n",
        "* token_type_ids: \n",
        "* attention_mask: \n",
        "* start_positions: \n",
        "* end_positions: \n",
        "\n",
        "* all_encoder_layers: \n",
        "* sequence_output: \n",
        "* logits: \n",
        "* start_logits: \n",
        "* end_logits: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6oCMjfbZDy5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def forward(self, input_ids, token_type_ids, attention_mask, start_positions=None, end_positions=None):\n",
        "        all_encoder_layers, _ = self.bert(input_ids, token_type_ids, attention_mask)\n",
        "        sequence_output = all_encoder_layers[-1]\n",
        "        logits = self.qa_outputs(sequence_output)\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1)\n",
        "        end_logits = end_logits.squeeze(-1)\n",
        "\n",
        "        if start_positions is not None and end_positions is not None:\n",
        "            # If we are on multi-GPU, split add a dimension - if not this is a no-op\n",
        "            start_positions = start_positions.squeeze(-1)\n",
        "            end_positions = end_positions.squeeze(-1)\n",
        "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
        "            ignored_index = start_logits.size(1)\n",
        "            start_positions.clamp_(0, ignored_index)\n",
        "            end_positions.clamp_(0, ignored_index)\n",
        "\n",
        "            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
        "            start_loss = loss_fct(start_logits, start_positions)\n",
        "            end_loss = loss_fct(end_logits, end_positions)\n",
        "            total_loss = (start_loss + end_loss) / 2\n",
        "            return total_loss\n",
        "        else:\n",
        "            return start_logits, end_logits"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}