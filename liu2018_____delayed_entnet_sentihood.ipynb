{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "liu2018_____delayed_entnet_sentihood.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMAMtCb1Vod5",
        "colab_type": "text"
      },
      "source": [
        "# 1. Delayed EntNet Sentihood (Constructor)<br>\n",
        "\n",
        "Parametros:\n",
        "\n",
        "* (atributo) batch_size: cantidad de ejemplos de entrenamiento utilizados en una iteracion\n",
        "* (atributo) vocab_size: cantidad de palabras en el vocabulario del archivo de embeddings\n",
        "* (atributo) target_len: tamaño de los objetivos (entidades)\n",
        "* (atributo) aspect_len: tamaño de los aspectos\n",
        "* (atributo) sentence_len: tamaño de la oracion mas larga del dataset\n",
        "* (atributo) answer_size: tamaño del vector de salida (3 porque las categorias son: positivo, negativo, ninguno)\n",
        "* (atributo) embedding_size: tamaño de los vectores de embedding\n",
        "* (atributo) embedding_mat: conjunto de vectores de embeddings\n",
        "* (atributo) update_embeddings: cambiar o no los embeddings (bool)\n",
        "* softmax_mask: aplicar una mascara o no a las operaciones con softmax\n",
        "* (atributo) max_grad_norm: para evitar que la gradiente crezca demasido y se produzcan NaN\n",
        "* (atributo) n_keys: numero de cadenas por cada objetivo (entidad) con embedding (key)\n",
        "* (atributo) tied_keys: los embeddings (keys) de los objetivos (entidades)\n",
        "* (atributo) l2_final_layer: lambda de L2 Norm (minimos cuadrados)\n",
        "* (atributo) initializer: forma de inicializar los pesos de la red neuronal\n",
        "* (atributo) optimizer: metodo para actualizar los pesos en cada iteracion (en vez de usar gradiente decendiente)\n",
        "* (atributo) session: sesion de tensorflow\n",
        "* (atributo) name: nombre del modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKl8MvPlTYIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Delayed_EntNet_Sentihood(object):\n",
        "    def __init__(self, \n",
        "        batch_size, vocab_size, target_len, aspect_len, sentence_len, \n",
        "        answer_size, embedding_size,\n",
        "        weight_tying=\"adj\",\n",
        "        hops=3,\n",
        "        embedding_mat=None,\n",
        "        update_embeddings=False,\n",
        "        softmax_mask=True,\n",
        "        max_grad_norm=5.0,\n",
        "        n_keys=6,\n",
        "        tied_keys=[],\n",
        "        l2_final_layer=0.0,\n",
        "        initializer=tf.contrib.layers.xavier_initializer(),\n",
        "        optimizer=tf.train.AdamOptimizer(learning_rate=1e-2),\n",
        "        global_step=None,\n",
        "        session=None,\n",
        "        name='Delayed_EntNet_Sentihood'):\n",
        "\n",
        "        print name\n",
        "\n",
        "        self._batch_size = batch_size\n",
        "        self._vocab_size = vocab_size\n",
        "        self._target_len = target_len\n",
        "        self._aspect_len = aspect_len\n",
        "        self._sentence_len = sentence_len\n",
        "        self._embedding_size = embedding_size\n",
        "        self._answer_size = answer_size\n",
        "        self._max_grad_norm = max_grad_norm\n",
        "        self._init = initializer\n",
        "        self._opt = optimizer\n",
        "        self._global_step = global_step\n",
        "        self._name = name\n",
        "        self._embedding_mat = embedding_mat\n",
        "        self._update_embeddings = update_embeddings\n",
        "\n",
        "        assert len(tied_keys) <= n_keys\n",
        "        self._n_keys = n_keys\n",
        "        self._tied_keys = tied_keys\n",
        "        self._l2_final_layer = l2_final_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9TsqXjigjZo",
        "colab_type": "text"
      },
      "source": [
        "Metodos:\n",
        "\n",
        "* build_inputs: crea los placeholders que el modelo va a usar (Seccion 2)\n",
        "* build_vars: crea las variables que el modelo va a usar (Seccion 3)\n",
        "* inference_adj: (Seccion 5)\n",
        "\n",
        "Atributos:\n",
        "\n",
        "* self._sentences: placeholder del tamaño mas grande de una oracion en el dataset\n",
        "* self._targets: placeholder del tamaño mas grande de un objetivo (entidad)\n",
        "* self._aspects: placeholder del tamaño mas grande de un aspecto\n",
        "* self._entnet_input_keep_prob: probabilidad de mantener un nodo en la entrada para evitar overfit\n",
        "* self._entnet_output_keep_prob: probabilidad de mantener un nodo en la salida para evitar overfit\n",
        "* self._entnet_state_keep_prob: probabilidad de mantener un nodo en los estados para evitar overfit\n",
        "* self._final_layer_keep_prob: probabilidad de mantener un nodo en la capa final para evitar overfit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgg6KLMsgSaV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "        self._build_inputs()\n",
        "        self._build_vars()\n",
        "\n",
        "        logits = self._inference_adj(\n",
        "            self._sentences, \n",
        "            self._targets,\n",
        "            self._aspects,\n",
        "            self._entnet_input_keep_prob,\n",
        "            self._entnet_output_keep_prob,\n",
        "            self._entnet_state_keep_prob,\n",
        "            self._final_layer_keep_prob,\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TO70z2giipFG",
        "colab_type": "text"
      },
      "source": [
        "* cross entropy:\n",
        "* logits: probabilidades no normalizadas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aie58HfEikLy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
        "            logits=logits, labels=tf.cast(self._answers_one_hot, tf.float32), \n",
        "            name=\"cross_entropy\"\n",
        "        )\n",
        "        cross_entropy_mean = tf.reduce_mean(\n",
        "            cross_entropy, name=\"cross_entropy_mean\"\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v61nUA1QifB3",
        "colab_type": "text"
      },
      "source": [
        "regularizacion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0mzs2sDiXYo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "        # l2 regularization\n",
        "        trainable_variables = tf.trainable_variables()\n",
        "        l2_loss_final_layer = 0.0\n",
        "        assert self._l2_final_layer >= 0\n",
        "\n",
        "        if self._l2_final_layer > 0:\n",
        "            final_layer_weights = [ tf.nn.l2_loss(v) for v in trainable_variables\n",
        "                                    if 'R:0' in v.name]\n",
        "            assert len(final_layer_weights) == 1\n",
        "            l2_loss_final_layer = self._l2_final_layer * tf.add_n(final_layer_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yANr82mzjbnO",
        "colab_type": "text"
      },
      "source": [
        "perdida"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URL3Iafpjdy8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "        # loss op\n",
        "        loss_op = cross_entropy_mean + l2_loss_final_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giwOrKsqjkAR",
        "colab_type": "text"
      },
      "source": [
        "gradiente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9edSha8jkNY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "        # gradient pipeline\n",
        "        grads_and_vars = self._opt.compute_gradients(loss_op)\n",
        "\n",
        "        grads_and_vars = [(tf.clip_by_norm(g, self._max_grad_norm), v) for g,v in grads_and_vars]\n",
        "        nil_grads_and_vars = []\n",
        "        \n",
        "        for g, v in grads_and_vars:\n",
        "            if v.name in self._nil_vars:\n",
        "                nil_grads_and_vars.append((zero_nil_slot(g), v))\n",
        "            else:\n",
        "                nil_grads_and_vars.append((g, v))\n",
        "        \n",
        "        train_op = self._opt.apply_gradients(nil_grads_and_vars, global_step=self._global_step, name=\"train_op\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd4e0PsbjrUi",
        "colab_type": "text"
      },
      "source": [
        "prediccion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_WUY8ivjrlc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "        # predict ops\n",
        "        predict_op = tf.argmax(logits, 1, name=\"predict_op\")\n",
        "        predict_proba_op = tf.nn.softmax(logits, name=\"predict_proba_op\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSn-gQdgjwvN",
        "colab_type": "text"
      },
      "source": [
        "Atributos:\n",
        "\n",
        "* self.loss_op = loss_op\n",
        "* self.predict_op = predict_op\n",
        "* self.predict_proba_op = predict_proba_op\n",
        "* self.train_op = train_op"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y7GELpNjw-n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "        # assign ops\n",
        "        self.loss_op = loss_op\n",
        "        self.predict_op = predict_op\n",
        "        self.predict_proba_op = predict_proba_op\n",
        "        self.train_op = train_op\n",
        "\n",
        "        init_op = tf.global_variables_initializer()\n",
        "        self._sess = session\n",
        "        self._sess.run(init_op, feed_dict={self._input_embedding: self._embedding_mat})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COeYdq9GV8X5",
        "colab_type": "text"
      },
      "source": [
        "# 2. Build inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wvp5P7VqdtH",
        "colab_type": "text"
      },
      "source": [
        "* (atributo) sentences: placeholder de tamaño variable x tamaño mas grande de una oracion en el dataset para guardar los indices de las palabras que lo conforman <br>\n",
        "Ejemplo sentihood: Max sentence len: 50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSjMwfAKUaCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def _build_inputs(self):\n",
        "        self._sentences = tf.placeholder(\n",
        "            tf.int32, [None, self._sentence_len], # None(tamaño variable) x tamaño maximo de una oracion\n",
        "            name=\"sentences\"\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDV9u-NtqilH",
        "colab_type": "text"
      },
      "source": [
        "* (atributo) targets: placeholder de tamaño variable x tamaño de un objetivo (entidad) para guardar los indices<br>\n",
        "Ejemplo sentihood: Max target size: 1 (LOCATION1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbH513FQp6pQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "        self._targets = tf.placeholder(\n",
        "            tf.int32, [None, self._target_len], # None(tamaño variable) x tamaño de un objetivo (entidad)\n",
        "            name=\"targets\"\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEpOnwLGqoDw",
        "colab_type": "text"
      },
      "source": [
        "* (atributo) aspects: placeholder de tamaño variable x tamaño de un aspecto para guardar los indices<br>\n",
        "Ejemplo sentihood: Max aspect size: 2 (transit-location)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTMOIlf9p_MV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "        self._aspects = tf.placeholder(\n",
        "            tf.int32, [None, self._aspect_len], # None(tamaño variable) x tamaño de un aspecto\n",
        "            name=\"aspects\"\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGEvJRtfqvQW",
        "colab_type": "text"
      },
      "source": [
        "* (atributo) answers: placeholder para los indices de las salidas (positivo, negativo, ninguno)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHYPKo58qByd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "        self._answers = tf.placeholder(\n",
        "            tf.int32, [None], # None(tamaño variable)\n",
        "            name=\"answers\"\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdBrl8YMqzes",
        "colab_type": "text"
      },
      "source": [
        "* (atributo) answers_one_hot: representacion one hot de las salidas (positivo, negativo, ninguno)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwjSa39GqMY1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "        self._answers_one_hot = tf.one_hot(\n",
        "            indices=self._answers, # largo variable\n",
        "            depth=self._answer_size, # ancho 3 (positivo negativo, ninguno)\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tRozVcArASb",
        "colab_type": "text"
      },
      "source": [
        "* (atributo) input_embedding: placeholder para el conjunto de vectores de embeddings\n",
        "\n",
        "```\n",
        "_embedding_mat.shape = (7, 3)\n",
        " array([[0., 1., 2.],\n",
        "       [1., 2., 3.],\n",
        "       [2., 3., 4.],\n",
        "       [3., 4., 5.],\n",
        "       [4., 5., 6.],\n",
        "       [5., 6., 7.],\n",
        "       [6., 7., 8.]], dtype=float32)\n",
        " ```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "av7s7fv-qOSf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "        self._input_embedding = tf.placeholder(\n",
        "            tf.float32, shape=self._embedding_mat.shape,\n",
        "            name=\"input_embedding\"\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTJFzaO-rE3b",
        "colab_type": "text"
      },
      "source": [
        "* (atributo) entnet_input_keep_prob: probabilidad de mantener un nodo al ajustar los pesos de una caracteristica para evitar overfit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snM45dnEqSw0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "        self._entnet_input_keep_prob = tf.placeholder(\n",
        "            tf.float32, shape=[], # shape=[] es un escalar\n",
        "            name=\"entnet_input_keep_prob\"\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rzyNUS1rMPl",
        "colab_type": "text"
      },
      "source": [
        "* (atributo) entnet_output_keep_prob: probabilidad de mantener un nodo en la salida de la unidad RNN para evitar overfit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_krFTrIZqU8t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "        self._entnet_output_keep_prob = tf.placeholder(\n",
        "            tf.float32, shape=[], # shape=[] es un escalar\n",
        "            name=\"entnet_output_keep_prob\"\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tH8dRJBrRsI",
        "colab_type": "text"
      },
      "source": [
        "* (atributo) entnet_state_keep_prob: probabilidad de mantener un nodo en el estado oculto que \"alimenta\" a la siguiente capa para evitar overfit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiPQCYjCqXCy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "        self._entnet_state_keep_prob = tf.placeholder(\n",
        "            tf.float32, shape=[], # shape=[] es un escalar\n",
        "            name=\"entnet_state_keep_prob\"\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2l7GoZmqrWA9",
        "colab_type": "text"
      },
      "source": [
        "* (atributo) final_layer_keep_prob: probabilidad de mantener un nodo en la capa final para evitar overfit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mpNTtJpqagE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "        self._final_layer_keep_prob = tf.placeholder(\n",
        "            tf.float32, shape=[], # shape=[] es un escalar\n",
        "            name=\"final_layer_keep_prob\"\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdZv_rt6WS85",
        "colab_type": "text"
      },
      "source": [
        "# 3. Build vars<br>\n",
        "\n",
        "Atributos:\n",
        "\n",
        "* self._embedding: variable del conjunto de vectores de embeddings\n",
        "* self._free_keys_embedding: variable con keys extra"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNBIrMIWU8oc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def _build_vars(self):\n",
        "        with tf.variable_scope(self._name):\n",
        "            self._embedding = tf.get_variable(\n",
        "                name=\"embedding\", # Nueva variable\n",
        "                dtype=tf.float32,\n",
        "                initializer=self._input_embedding, # Conjunto de vectores de embeddings\n",
        "                trainable=self._update_embeddings, # Cambiar o no los embeddings (bool)\n",
        "            )\n",
        "\n",
        "            self._free_keys_embedding = tf.get_variable(\n",
        "                name=\"free_keys_embedding\", # Nueva variable\n",
        "                dtype=tf.float32,\n",
        "                shape=[self._n_keys - len(self._tied_keys), self._embedding_size], # keys extra\n",
        "                initializer=self._init, # Inicializador de pesos\n",
        "                trainable=True,\n",
        "            )\n",
        "\n",
        "        self._nil_vars = set([self._embedding.name])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNkUbv5FWa25",
        "colab_type": "text"
      },
      "source": [
        "# 4. Mask embedding<br>\n",
        "\n",
        "Parametros:\n",
        " \n",
        " * embedding: conjunto de vectores de embeddings (vocab_size=7 x embedding_size=3)\n",
        "\n",
        "```\n",
        " array([[0., 1., 2.],\n",
        "       [1., 2., 3.],\n",
        "       [2., 3., 4.],\n",
        "       [3., 4., 5.],\n",
        "       [4., 5., 6.],\n",
        "       [5., 6., 7.],\n",
        "       [6., 7., 8.]], dtype=float32)\n",
        " ```\n",
        " \n",
        "Variables:\n",
        " \n",
        "* vocab_size: la cantidad de elementos que existen en el vocabulario de embeddings\n",
        "* embedding_size: el tamaño de los vectores de embedding\n",
        "\n",
        "Constantes:\n",
        "* embedding_mask: mascara de un cero y unos (vocab_size=7 x 1)\n",
        "\n",
        "```\n",
        "array([[0.],\n",
        "       [1.],\n",
        "       [1.],\n",
        "       [1.],\n",
        "       [1.],\n",
        "       [1.],\n",
        "       [1.]], dtype=float32)\n",
        "\n",
        "```\n",
        "\n",
        "Retorna:\n",
        "\n",
        "```\n",
        "array([[0., 0., 0.],\n",
        "       [1., 2., 3.],\n",
        "       [2., 3., 4.],\n",
        "       [3., 4., 5.],\n",
        "       [4., 5., 6.],\n",
        "       [5., 6., 7.],\n",
        "       [6., 7., 8.]], dtype=float32)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLxpZZoQVDsL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def _mask_embedding(self, embedding):\n",
        "        vocab_size, embedding_size = self._embedding_mat.shape\n",
        "        embedding_mask = tf.constant(\n",
        "            value=[0 if i == 0 else 1 for i in range(vocab_size)], # El primer elemento es cero, los demas son unos\n",
        "            shape=[vocab_size, 1],\n",
        "            dtype=tf.float32,\n",
        "            name=\"embedding_mask\",\n",
        "        )\n",
        "        return embedding * embedding_mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peMBRbcbWniZ",
        "colab_type": "text"
      },
      "source": [
        "# 5. Inference adj<br>\n",
        "\n",
        "Metodos:\n",
        "\n",
        "* mask_embedding: Aplica una mascara de unos y ceros al conjunto de embeddings (Seccion 4)\n",
        "\n",
        "Parametros:\n",
        "\n",
        "* sentences: placeholder de tamaño variable x tamaño mas grande de una oracion en el dataset para guardar los indices\n",
        "* targets: placeholder de tamaño variable x tamaño de un objetivo (entidad) para guardar los indices\n",
        "* aspects: placeholder de tamaño variable x tamaño de un aspecto para guardar los indices\n",
        "* entnet_input_keep_prob: probabilidad de mantener un nodo en la entrada para evitar overfit\n",
        "* entnet_output_keep_prob: probabilidad de mantener un nodo en la salida para evitar overfit\n",
        "* entnet_state_keep_prob: probabilidad de mantener un nodo en los estados para evitar overfit\n",
        "* final_layer_keep_prob: probabilidad de mantener un nodo en la capa final para evitar overfit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jg0O94gHVSdw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def _inference_adj(self, sentences, targets, aspects, \n",
        "                       entnet_input_keep_prob, entnet_output_keep_prob, \n",
        "                       entnet_state_keep_prob, final_layer_keep_prob):\n",
        "    \n",
        "        with tf.variable_scope(self._name):\n",
        "            masked_embedding = self._mask_embedding(self._embedding)\n",
        "\n",
        "            batch_size = tf.shape(sentences)[0] # tf.shape retorna un vector con la 'forma', en la posicion [0] esta "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cBMv_N3OnyM",
        "colab_type": "text"
      },
      "source": [
        "* tf.nn.embedding_lookup: Se busca el embedding de los objetivos (entidades) en el conjunto de embeddings\n",
        "* reduce_mean: reduce los vectores de targets_emb a su promedio en caso de objetivos (entidades) compuestos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wcw_xkhAuEEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "            targets_emb = tf.nn.embedding_lookup(masked_embedding, targets) # targets contiene los indices\n",
        "            # [None, entity_size, emb_size]\n",
        "            \n",
        "            targets_emb = tf.reduce_mean(\n",
        "                input_tensor=targets_emb,\n",
        "                axis=1,\n",
        "                keep_dims=True,\n",
        "            )\n",
        "            # [None, 1, emb_size]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XI0b8k4prkO",
        "colab_type": "text"
      },
      "source": [
        "* tf.nn.embedding_lookup: Se busca el embedding de los aspectos en el conjunto de embeddings\n",
        "* reduce_mean: reduce los vectores de aspects_emb a su promedio en caso de aspectos compuestos (transit-location)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWkw5TiKuRuX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "            aspects_emb = tf.nn.embedding_lookup(masked_embedding, aspects) # aspects contiene los indices\n",
        "            # [None, aspect_size, emb_size]\n",
        "            \n",
        "            aspects_emb = tf.reduce_mean(\n",
        "                input_tensor=aspects_emb,\n",
        "                axis=1,\n",
        "                keep_dims=True,\n",
        "            )\n",
        "            # [None, 1, emb_size]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUlxBargr2JB",
        "colab_type": "text"
      },
      "source": [
        "* tf.nn.embedding_lookup: Se busca el embedding de las oraciones en el conjunto de embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-TwhTkdufiK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "            sentences_emb = tf.nn.embedding_lookup(masked_embedding, sentences) # sentences contiene los indices\n",
        "            # [None, memory_size, emb_size]\n",
        "    \n",
        "            sentences_len = self._sentence_length(sentences_emb)\n",
        "            # [None]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ueKDtnGC77W",
        "colab_type": "text"
      },
      "source": [
        "* tf.nn.embedding_lookup: Se busca el embedding de los key (objetivo) de cada cadena en el conjunto de embeddings\n",
        "* reduce_mean: reduce los vectores de tied_keys_emb a su promedio en caso de objetivos compuestos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kg036mv1u1dD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "            tied_keys_emb = tf.nn.embedding_lookup(masked_embedding, self._tied_keys)\n",
        "            # [len(self._tied_keys), max_key_len, emb_size]\n",
        "            \n",
        "            tied_keys_emb = tf.reduce_mean(\n",
        "                input_tensor=tied_keys_emb,\n",
        "                axis=1,\n",
        "            )\n",
        "            # [len(self._tied_keys), emb_size]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fN5-SlhDkkqi",
        "colab_type": "text"
      },
      "source": [
        "* free_keys_emb: variable con keys (objetivos) extra"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8Z6EA8Gu6Tn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "            free_keys_emb = self._free_keys_embedding\n",
        "            # [n_keys - len(self._tied_keys), emb_size]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0x_GEClPPxMZ",
        "colab_type": "text"
      },
      "source": [
        "* tf.concat: uniendo las variables de los key (objetivo) \n",
        "\n",
        "```\n",
        "t1 = [[1, 2, 3], [4, 5, 6]]\n",
        "t2 = [[7, 8, 9], [10, 11, 12]]\n",
        "tf.concat([t1, t2], axis=0)  # [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CD5xnbiwvC9D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "            keys_emb = tf.concat(\n",
        "                values=[tied_keys_emb, free_keys_emb],\n",
        "                axis=0,\n",
        "                name=\"keys_emb\",\n",
        "            )\n",
        "            # [n_keys, emb_size]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pt86UC_6MDfo",
        "colab_type": "text"
      },
      "source": [
        "* tf.tile: replicando los key (objetivos) para cada elemento del batch\n",
        "\n",
        "```\n",
        "batch_size = 3\n",
        "\n",
        "keys_emb =\n",
        "[[ 1.,  2.],\n",
        " [ 2.,  1.]]\n",
        "\n",
        "tf.tile =\n",
        "[[[ 1.,  2.],\n",
        "  [ 2.,  1.]],\n",
        "\n",
        " [[ 1.,  2.],\n",
        "  [ 2.,  1.]],\n",
        "\n",
        " [[ 1.,  2.],\n",
        "  [ 2.,  1.]]]\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgIvhcGNvIi-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "            batched_keys_emb = tf.tile(\n",
        "                input=tf.expand_dims(input=keys_emb, axis=0),\n",
        "                multiples=[batch_size, 1, 1]\n",
        "            )\n",
        "            # [None, n_keys, emb_size]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_KEncp9kMC4",
        "colab_type": "text"
      },
      "source": [
        "* tf.split: se coloca cada key en vectores independientes\n",
        "\n",
        "```\n",
        "[array([[1., 2.]], dtype=float32),\n",
        " array([[2., 1.]], dtype=float32)]\n",
        "```\n",
        "\n",
        "* tf.squeeze: se redimensiona el vector de keys para eliminar dimensiones extra\n",
        "\n",
        "```\n",
        "keys=[[1., 2.],\n",
        "      [2., 1.]]\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LINROTYvVrC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "            keys = tf.split(keys_emb, self._n_keys, axis=0)\n",
        "            # list of [1, emb_size]\n",
        "    \n",
        "            keys = [tf.squeeze(key, axis=0) for key in keys]\n",
        "            # list of [emb_size]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9VFAZrJ5ZFr",
        "colab_type": "text"
      },
      "source": [
        "* partial: crea una nueva funcion a partir de otra con parametros pre establecidos\n",
        "\n",
        "```\n",
        "def func(a,b,c,d):\n",
        "    return a + b + c + d\n",
        "func2 = partial(func, a='1', b='2', c='3')\n",
        "print(func2('4')) # 1234\n",
        "```\n",
        "\n",
        "* activation: se usa la funcion de activacion PReLU (Seccion 18)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vF_EQ7NdvtHs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "            alpha = tf.get_variable(\n",
        "                name='alpha', # Nueva variable\n",
        "                shape=self._embedding_size, # Tamaño de los vectores de embedding\n",
        "                initializer=tf.constant_initializer(1.0)\n",
        "            ) \n",
        "    \n",
        "            activation = partial(prelu, alpha=alpha)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13ILrBY0jnwQ",
        "colab_type": "text"
      },
      "source": [
        "Se crea la celda de Memoria Dinamica para recorrer la oracion de izquierda a derecha\n",
        "\n",
        "* DynamicMemoryCell: crea una celda de Memoria Dinamica (Seccion 10)\n",
        "* num_blocks: cantidad de keys\n",
        "* num_units_per_block: tamaño de los vectores de embeddings\n",
        "* keys: embedings de los keys\n",
        "* initializer: inicializador de pesos\n",
        "* recurrent_initializer: inicializador de pesos\n",
        "* activation: funcion de activacion (PReLU)\n",
        "\n",
        "* zero_state: crea la forma (shape) del estado\n",
        "* sentences_emb_shape: dimensiones del vector con las entradas (oraciones)\n",
        "* cell_fw: celda de Memoria Dinamica\n",
        "* input_keep_prob: probabilidad de dropout al ajustar los pesos de una caracteristica\n",
        "* output_keep_prob: probabilidad de dropout para cada salida de la unidad RNN\n",
        "* state_keep_prob: probabilidad de dropout para el estado oculto que \"alimenta\" a la siguiente capa\n",
        "* input_size: tamaño del embedding de las palabras en la oracion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mI3AimCywCMX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "            cell_fw = DynamicMemoryCell(\n",
        "                num_blocks=self._n_keys,\n",
        "                num_units_per_block=self._embedding_size,\n",
        "                keys=keys,\n",
        "                initializer=self._init,\n",
        "                recurrent_initializer=self._init,\n",
        "                activation=activation,\n",
        "            )\n",
        "  \n",
        "            initial_state_fw = cell_fw.zero_state(batch_size, tf.float32)\n",
        "            sentences_emb_shape = sentences_emb.get_shape()\n",
        "      \n",
        "            cell_fw = tf.contrib.rnn.DropoutWrapper(\n",
        "                cell=cell_fw,\n",
        "                input_keep_prob=entnet_input_keep_prob,\n",
        "                output_keep_prob=entnet_output_keep_prob,\n",
        "                state_keep_prob=entnet_state_keep_prob,\n",
        "                variational_recurrent=True,\n",
        "                input_size=(sentences_emb_shape[2]),\n",
        "                dtype=tf.float32,\n",
        "            )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAfeGB-sI1Qw",
        "colab_type": "text"
      },
      "source": [
        "Se crea la celda de Memoria Dinamica para recorrer la oracion de derecha a izquierda\n",
        "\n",
        "* DynamicMemoryCell: crea una celda de Memoria Dinamica (Seccion 10)\n",
        "* num_blocks: cantidad de keys\n",
        "* num_units_per_block: tamaño de los vectores de embeddings\n",
        "* keys: embedings de los keys\n",
        "* initializer: inicializador de pesos\n",
        "* recurrent_initializer: inicializador de pesos\n",
        "* activation: funcion de activacion (PReLU)\n",
        "\n",
        "* zero_state: crea la forma (shape) del estado\n",
        "* sentences_emb_shape: dimensiones del vector con las entradas (oraciones)\n",
        "* cell_bw: celda de Memoria Dinamica\n",
        "* input_keep_prob: probabilidad de dropout al ajustar los pesos de una caracteristica\n",
        "* output_keep_prob: probabilidad de dropout para cada salida de la unidad RNN\n",
        "* state_keep_prob: probabilidad de dropout para el estado oculto que \"alimenta\" a la siguiente capa\n",
        "* input_size: tamaño del embedding de las palabras en la oracion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDPS6VQZwoki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "            cell_bw = DynamicMemoryCell(\n",
        "                num_blocks=self._n_keys,\n",
        "                num_units_per_block=self._embedding_size,\n",
        "                keys=keys,\n",
        "                initializer=self._init,\n",
        "                recurrent_initializer=self._init,\n",
        "                activation=activation,\n",
        "            )\n",
        "  \n",
        "            initial_state_bw = cell_bw.zero_state(batch_size, tf.float32)\n",
        "    \n",
        "            cell_bw = tf.contrib.rnn.DropoutWrapper(\n",
        "                cell=cell_bw,\n",
        "                input_keep_prob=entnet_input_keep_prob,\n",
        "                output_keep_prob=entnet_output_keep_prob,\n",
        "                state_keep_prob=entnet_state_keep_prob,\n",
        "                variational_recurrent=True,\n",
        "                input_size=(sentences_emb_shape[2]),\n",
        "                dtype=tf.float32,\n",
        "            )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6vZj-n9JpcV",
        "colab_type": "text"
      },
      "source": [
        "Se unen las dos celdas de Memoria Dinamica (izquierda a derecha y derecha a izquierda) y se obtienen los ultimos estados de estas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV_z8P0XxGqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "            (_, _), (last_state_fw, last_state_bw) = tf.nn.bidirectional_dynamic_rnn(\n",
        "                cell_fw=cell_fw,\n",
        "                cell_bw=cell_bw,\n",
        "                inputs=sentences_emb,\n",
        "                sequence_length=sentences_len,\n",
        "                initial_state_fw=initial_state_fw,\n",
        "                initial_state_bw=initial_state_bw,\n",
        "            )\n",
        "\n",
        "            last_state_fw, _ = tf.split(\n",
        "                value=last_state_fw,\n",
        "                num_or_size_splits=[\n",
        "                    self._n_keys * self._embedding_size, \n",
        "                    self._n_keys * self._embedding_size,\n",
        "                ],\n",
        "                axis=1\n",
        "            )\n",
        "    \n",
        "            last_state_bw, _ = tf.split(\n",
        "                value=last_state_bw,\n",
        "                num_or_size_splits=[\n",
        "                    self._n_keys * self._embedding_size, \n",
        "                    self._n_keys * self._embedding_size,\n",
        "                ],\n",
        "                axis=1\n",
        "            )\n",
        "            # last_state_f/bw: [None, emb_size * n_keys]\n",
        "            \n",
        "            last_state_fw = tf.stack(\n",
        "                tf.split(last_state_fw, self._n_keys, axis=1), axis=1)\n",
        "            # [None, n_keys, emb_size]\n",
        "            \n",
        "            last_state_bw = tf.stack(\n",
        "                tf.split(last_state_bw, self._n_keys, axis=1), axis=1)\n",
        "            # [None, n_keys, emb_size]\n",
        "            \n",
        "            last_state = last_state_fw + last_state_bw\n",
        "            # [None, n_keys, emb_size]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bUd5MvwMyfz",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://i.ibb.co/y6fTK0q/liu2018-output.png\" alt=\"liu2018-output\" border=\"0\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpum-YLFxaXH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "            asp_att = tf.concat(values=[targets_emb, aspects_emb], axis=2)\n",
        "            # [None, 1, emb_size * 2]\n",
        "      \n",
        "            W_asp_att = tf.get_variable(\n",
        "                name='W_asp_att',\n",
        "                shape=[self._embedding_size, self._embedding_size * 2],\n",
        "                dtype=tf.float32,\n",
        "                initializer=self._init,\n",
        "            )\n",
        "        \n",
        "            temp = tf.tensordot(\n",
        "                batched_keys_emb, W_asp_att, [[2], [0]]\n",
        "            )\n",
        "            # [None, n_keys, emb_size * 2]\n",
        "            \n",
        "            attention = tf.reduce_sum(temp * asp_att, axis=2)\n",
        "            # [None, n_keys]\n",
        "            \n",
        "            attention_max = tf.reduce_max(attention, axis=-1, keep_dims=True)\n",
        "            # [None, 1]\n",
        "            \n",
        "            attention = tf.nn.softmax(attention - attention_max)\n",
        "            # [None, n_keys]\n",
        "            \n",
        "            attention = tf.expand_dims(attention, axis=2)\n",
        "            # [None, n_keys, 1]\n",
        "\n",
        "            u = tf.reduce_sum(last_state * attention, axis=1)\n",
        "            # [None, emb_size]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kO0TJhBOYpk",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://i.ibb.co/f2njYfY/liu2018-y.png\" alt=\"liu2018-y\" border=\"0\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfFh5f-nOI05",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "            R = tf.get_variable('R', [self._embedding_size, self._answer_size])\n",
        "            H = tf.get_variable('H', [self._embedding_size, self._embedding_size])\n",
        "\n",
        "            a = tf.squeeze(aspects_emb, axis=1)\n",
        "            # [None, emb_size]\n",
        "            \n",
        "            hidden = activation(a + tf.matmul(u, H))\n",
        "            # [None, emb_size]\n",
        "            \n",
        "            hidden = tf.nn.dropout(x=hidden, keep_prob=final_layer_keep_prob)\n",
        "            # [None, emb_size]\n",
        "            \n",
        "            y = tf.matmul(hidden, R)\n",
        "            # [None, 1] -> El elemento unico (1) es un vector de (1x3) para las categorias (positivo, negativo, ninguno)\n",
        "\n",
        "            return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvOXmWbqW4Ki",
        "colab_type": "text"
      },
      "source": [
        "# 6. Get mini batch start end<br>\n",
        "\n",
        "Parametros:\n",
        "\n",
        "* n_train: \n",
        "* batch_size: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aSa4dj_VYtJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def _get_mini_batch_start_end(self, n_train, batch_size=None):\n",
        "        '''\n",
        "        Args:\n",
        "            n_train: int, number of training instances\n",
        "            batch_size: int (or None if full batch)\n",
        "        \n",
        "        Returns:\n",
        "            batches: list of tuples of (start, end) of each mini batch\n",
        "        '''\n",
        "        mini_batch_size = n_train if batch_size is None else batch_size\n",
        "        batches = zip(\n",
        "            range(0, n_train, mini_batch_size),\n",
        "            list(range(mini_batch_size, n_train, mini_batch_size)) + [n_train]\n",
        "        )\n",
        "        return batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8rltyazW-XL",
        "colab_type": "text"
      },
      "source": [
        "# 7. Fit\n",
        "\n",
        "Parametros:\n",
        "\n",
        "* sentences: placeholder del tamaño mas grande de una oracion en el dataset\n",
        "* targets: placeholder del tamaño mas grande de un objetivo (entidad)\n",
        "* aspects: placeholder del tamaño mas grande de un aspecto\n",
        "* entnet_input_keep_prob: probabilidad de mantener un nodo en la entrada para evitar overfit\n",
        "* entnet_output_keep_prob: probabilidad de mantener un nodo en la salida para evitar overfit\n",
        "* entnet_state_keep_prob: probabilidad de mantener un nodo en los estados para evitar overfit\n",
        "* final_layer_keep_prob: probabilidad de mantener un nodo en la capa final para evitar overfit\n",
        "* batch_size: cantidad de ejemplos de entrenamiento utilizados en una iteracion\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlM2C82VVcbS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def fit(self, sentences, targets, aspects, answers, entnet_input_keep_prob, \n",
        "            entnet_output_keep_prob, entnet_state_keep_prob, \n",
        "            final_layer_keep_prob, batch_size=None):\n",
        "    \n",
        "        assert len(sentences) == len(targets)\n",
        "        assert len(sentences) == len(aspects)\n",
        "        assert len(sentences) == len(answers)\n",
        "        \n",
        "        batches = self._get_mini_batch_start_end(len(sentences), batch_size)\n",
        "        total_loss = 0.\n",
        "        \n",
        "        for start, end in batches:\n",
        "            feed_dict = {\n",
        "                self._sentences: sentences[start:end], \n",
        "                self._targets: targets[start:end],\n",
        "                self._aspects: aspects[start:end],\n",
        "                self._answers: answers[start:end], \n",
        "                self._entnet_input_keep_prob: entnet_input_keep_prob,\n",
        "                self._entnet_output_keep_prob: entnet_output_keep_prob,\n",
        "                self._entnet_state_keep_prob: entnet_state_keep_prob,\n",
        "                self._final_layer_keep_prob: final_layer_keep_prob,\n",
        "            }\n",
        "            loss, _ = self._sess.run(\n",
        "                [self.loss_op, self.train_op], \n",
        "                feed_dict=feed_dict\n",
        "            )\n",
        "            total_loss = loss * len(sentences[start:end])\n",
        "        return total_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FU3keGNPXEjr",
        "colab_type": "text"
      },
      "source": [
        "# 8. Predict<br>\n",
        "\n",
        "Parametros:\n",
        "\n",
        "* sentences: placeholder del tamaño mas grande de una oracion en el dataset\n",
        "* targets: placeholder del tamaño mas grande de un objetivo (entidad)\n",
        "* aspects: placeholder del tamaño mas grande de un aspecto\n",
        "* batch_size: cantidad de ejemplos de prueba utilizados en una iteracion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDtdZcZ7Ve4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def predict(self, sentences, targets, aspects, batch_size=None):\n",
        "        assert len(sentences) == len(targets)\n",
        "        assert len(sentences) == len(aspects)\n",
        "        \n",
        "        batches = self._get_mini_batch_start_end(len(sentences), batch_size)\n",
        "        predictions, predictions_prob = [], []\n",
        "        \n",
        "        for start, end in batches:\n",
        "            feed_dict = {\n",
        "                self._sentences: sentences[start:end], \n",
        "                self._targets: targets[start:end],\n",
        "                self._aspects: aspects[start:end],\n",
        "                self._entnet_input_keep_prob: 1.0,\n",
        "                self._entnet_output_keep_prob: 1.0,\n",
        "                self._entnet_state_keep_prob: 1.0,\n",
        "                self._final_layer_keep_prob: 1.0,\n",
        "            }\n",
        "            \n",
        "            prediction, prediction_prob = self._sess.run(\n",
        "                [self.predict_op, self.predict_proba_op],\n",
        "                feed_dict=feed_dict\n",
        "            )\n",
        "            \n",
        "            predictions.extend(prediction)\n",
        "            predictions_prob.extend(prediction_prob)\n",
        "            \n",
        "        return predictions, np.array(predictions_prob)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6T9KjO5lXOta",
        "colab_type": "text"
      },
      "source": [
        "# 9. Sentence length<br>\n",
        "\n",
        "Parametros:\n",
        "\n",
        "* sentences: oracion transformada en vectores de embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4KNn_UoVh1R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def _sentence_length(self, sentences):\n",
        "        '''\n",
        "        sentences: (None, sentence_len, embedding_size)\n",
        "        '''\n",
        "        used = tf.sign(tf.reduce_max(tf.abs(sentences), reduction_indices=2))\n",
        "        length = tf.reduce_sum(used, reduction_indices=1)\n",
        "        length = tf.cast(length, tf.int32)\n",
        "        return length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5Ms_LK9XkPB",
        "colab_type": "text"
      },
      "source": [
        "# =================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxEOUG7qX7-a",
        "colab_type": "text"
      },
      "source": [
        "# 10. Dynamic Memory Cell (Constructor)<br>\n",
        "\n",
        "Parametros:\n",
        "\n",
        "* (atributo) num_blocks: Cantidad de cadenas\n",
        "* (atributo) num_units_per_block: Tamaño del vector de embeddings\n",
        "* (atributo) keys: Embeddings de los keys de cada cadena\n",
        "* (atributo) initializer: Forma de inicializar los pesos de la red neuronal\n",
        "* (atributo) recurrent_initializer: Forma de inicializar los pesos de la red neuronal\n",
        "* (atributo) activation: Funcion de activacion (phi) ReLU\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFbCn1cPYNjZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DynamicMemoryCell(tf.contrib.rnn.RNNCell):\n",
        "    \"\"\"\n",
        "    Implementation of a dynamic memory cell as a gated recurrent network.\n",
        "    The cell's hidden state is divided into blocks and each block's weights are tied.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_blocks,\n",
        "                 num_units_per_block,\n",
        "                 keys,\n",
        "                 initializer=None,\n",
        "                 recurrent_initializer=None,\n",
        "                 activation=tf.nn.relu,):\n",
        "        \n",
        "        self._num_blocks = num_blocks # M\n",
        "        self._num_units_per_block = num_units_per_block # d\n",
        "        self._keys = keys\n",
        "        self._activation = activation # \\phi\n",
        "        self._initializer = initializer\n",
        "        self._recurrent_initializer = recurrent_initializer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIfg5kj0ZCyG",
        "colab_type": "text"
      },
      "source": [
        "# 11. State size\n",
        "\n",
        "Retorna el tamaño total de la celda"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsAg8T4vYkqB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    @property\n",
        "    def state_size(self):\n",
        "        return self._num_blocks * self._num_units_per_block * 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0UT_-UEZF4b",
        "colab_type": "text"
      },
      "source": [
        "# 12. Output size\n",
        "\n",
        "Retorna el tamaño total de la salida de la celda"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p93IDKnLYpB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    @property\n",
        "    def output_size(self):\n",
        "        return self._num_blocks * self._num_units_per_block"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aNxCySHZI8l",
        "colab_type": "text"
      },
      "source": [
        "# 13. Zero state<br>\n",
        "\n",
        "Parametros:\n",
        "\n",
        "* batch_size: cantidad de ejemplos de entrenamiento utilizados en una iteracion\n",
        "\n",
        "```\n",
        "keys = [[1., 1., 1., 1.], [2., 2., 2., 2.]]\n",
        "zero_state = [1., 1., 1., 1., 2., 2., 2., 2.]\n",
        "batch_size = 5\n",
        "\n",
        "zero_state_batch =[[1., 1., 1., 1., 2., 2., 2., 2.],\n",
        "                   [1., 1., 1., 1., 2., 2., 2., 2.],\n",
        "                   [1., 1., 1., 1., 2., 2., 2., 2.],\n",
        "                   [1., 1., 1., 1., 2., 2., 2., 2.],\n",
        "                   [1., 1., 1., 1., 2., 2., 2., 2.]]\n",
        "\n",
        "num_blocks = 2\n",
        "num_units_per_block = 4\n",
        "\n",
        "return = [[1., 1., 1., 1., 2., 2., 2., 2., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
        "          [1., 1., 1., 1., 2., 2., 2., 2., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
        "          [1., 1., 1., 1., 2., 2., 2., 2., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
        "          [1., 1., 1., 1., 2., 2., 2., 2., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
        "          [1., 1., 1., 1., 2., 2., 2., 2., 0., 0., 0., 0., 0., 0., 0., 0.]]\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6MCGTAiYuYp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def zero_state(self, batch_size, dtype):\n",
        "        \"Initialize the memory to the key values.\"\n",
        "        zero_state = tf.concat([tf.expand_dims(key, axis=0) for key in self._keys], axis=1)\n",
        "        zero_state_batch = tf.tile(zero_state, [batch_size, 1])\n",
        "        \n",
        "        return tf.concat(\n",
        "            values=[\n",
        "                zero_state_batch,\n",
        "                tf.zeros(\n",
        "                    shape=[batch_size, self._num_blocks * self._num_units_per_block],\n",
        "                    dtype=tf.float32,\n",
        "                ),\n",
        "            ],\n",
        "            axis=1\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVvVtt4lZQMM",
        "colab_type": "text"
      },
      "source": [
        "# 14. Get gate<br>\n",
        "\n",
        "Parametros:\n",
        "\n",
        "* state_j: memoria anterior a la actual\n",
        "* key_j: embedding del objetivo (entidad) de la cadena de memorias\n",
        "* inputs: palabras de la oracion de entrada\n",
        "* v: vector entrenable\n",
        "* prev_a: activacion previa\n",
        "\n",
        "<img src=\"https://i.ibb.co/nQgMrKq/liu2018-eq2.png\" alt=\"liu2018-eq2\" border=\"0\">\n",
        "\n",
        "# $a  = w_{i} * h^{j}_{i-1} $\n",
        "# $b  = w_{i} * k^{j} $\n",
        "# $c  = v * d^{j}_{i} $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "berjRWiDV-Xh",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "batch_size = 5\n",
        "embedding_size = 4\n",
        "\n",
        "a = inputs(4x1) * state_j(1x4) = (4x4)\n",
        "b = inputs(4x1) * key_j(1x4) = (4x4)\n",
        "c = prev_a(4x1) * v(1x4) = (4x4)\n",
        "\n",
        "a + b + c = (4x4)\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PGO6kKIYy3U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def get_gate(self, state_j, key_j, inputs, v=None, prev_a=None):\n",
        "        a = tf.reduce_sum(inputs * state_j, axis=1)\n",
        "        b = tf.reduce_sum(inputs * key_j, axis=1)\n",
        "        assert v is not None\n",
        "        c = tf.reduce_sum(prev_a * v, axis=1)\n",
        "        return tf.sigmoid(a + b + c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmn3BbcBZU5r",
        "colab_type": "text"
      },
      "source": [
        "# 15. Get candidate<br>\n",
        "\n",
        "Parametros:\n",
        "\n",
        "* state_j: memoria anterior a la actual\n",
        "* key_j: embedding del objetivo (entidad) de la cadena de memorias\n",
        "* inputs: palabras de la oracion de entrada\n",
        "* U, V y W: matrices de parametros entrenables\n",
        "* U_bias: bias de la matriz de parametros entrenable U\n",
        "\n",
        "<img src=\"https://i.ibb.co/x7Fk6GC/liu2018-eq3y4.png\" alt=\"liu2018-eq3y4\" border=\"0\">\n",
        "\n",
        "# $ key = V k^{j} $\n",
        "# $ state = U h^{j}_{i-1} $\n",
        "# $ inputs = W w_{i} $\n",
        "\n",
        "```\n",
        "num_units_per_block = 4\n",
        "\n",
        "key = V(4x4) * k(4x1) = (4x1)\n",
        "state = U(4x4) * h(4x1) = (4x1) + bias(4x1) = (4x1)\n",
        "inputs = W(4x4) * w(4x1) = (4x1)\n",
        "\n",
        "state + inputs + key = (4x1)\n",
        "\n",
        "return = [[15.],\n",
        "          [15.],\n",
        "          [15.],\n",
        "          [15.]]\n",
        "\n",
        "```\n",
        "\n",
        "* U, V y W: matrices de tamaño (num_units_per_block x num_units_per_block)\n",
        "* U_bias: bias de tamano (num_units_per_block x 1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWDGbNTiY25j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def get_candidate(self, state_j, key_j, inputs, U, V, W, U_bias):\n",
        "        key_V = tf.matmul(key_j, V)\n",
        "        state_U = tf.matmul(state_j, U) + U_bias\n",
        "        inputs_W = tf.matmul(inputs, W)\n",
        "        \n",
        "        return self._activation(state_U + inputs_W + key_V)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xpD0FPFZYck",
        "colab_type": "text"
      },
      "source": [
        "# 16. Dynamic Memory Cell (Funcion)\n",
        "\n",
        "Parametros:\n",
        "\n",
        "* inputs: palabras de la oracion de entrada\n",
        "* state: memoria\n",
        "* U, V y W: matrices de tamaño (num_units_per_block x num_units_per_block)\n",
        "* U_bias: bias de tamano (num_units_per_block x 1)\n",
        "* num_units_per_block: tamaño del vector de embeddings\n",
        "* recurrent_initializer: forma de inicializar los pesos\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VMMx2PjY8a6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def __call__(self, inputs, state, scope=None):\n",
        "        with tf.variable_scope(scope or type(self).__name__, initializer=self._initializer):\n",
        "            U = tf.get_variable('U', [self._num_units_per_block, self._num_units_per_block],\n",
        "                                initializer=self._recurrent_initializer)\n",
        "            V = tf.get_variable('V', [self._num_units_per_block, self._num_units_per_block],\n",
        "                                initializer=self._recurrent_initializer)\n",
        "            W = tf.get_variable('W', [self._num_units_per_block, self._num_units_per_block],\n",
        "                                initializer=self._recurrent_initializer)\n",
        "\n",
        "            U_bias = tf.get_variable('U_bias', [self._num_units_per_block])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIn6pQmBCMjg",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "batch_size = 5\n",
        "embedding_size = 4\n",
        "num_blocks = 2\n",
        "num_units_per_block = embedding_size = 4\n",
        "\n",
        "state = [[1., 1., 1., 1., 2., 2., 2., 2., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
        "         [1., 1., 1., 1., 2., 2., 2., 2., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
        "         [1., 1., 1., 1., 2., 2., 2., 2., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
        "         [1., 1., 1., 1., 2., 2., 2., 2., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
        "         [1., 1., 1., 1., 2., 2., 2., 2., 0., 0., 0., 0., 0., 0., 0., 0.]]\n",
        "         \n",
        "state = [[1., 1., 1., 1., 2., 2., 2., 2.],\n",
        "         [1., 1., 1., 1., 2., 2., 2., 2.],\n",
        "         [1., 1., 1., 1., 2., 2., 2., 2.],\n",
        "         [1., 1., 1., 1., 2., 2., 2., 2.],\n",
        "         [1., 1., 1., 1., 2., 2., 2., 2.]]\n",
        "\n",
        "state_a = [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
        "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
        "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
        "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
        "           [0., 0., 0., 0., 0., 0., 0., 0.]]\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMj6-IF70xlw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "            state, state_a = tf.split(\n",
        "                value=state,\n",
        "                num_or_size_splits=[\n",
        "                    self._num_blocks * self._num_units_per_block,\n",
        "                    self._num_blocks * self._num_units_per_block\n",
        "                ],\n",
        "                axis=1,\n",
        "            )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA64YpAXEpD7",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "state_a = [array([[0., 0., 0., 0.],\n",
        "                  [0., 0., 0., 0.],\n",
        "                  [0., 0., 0., 0.],\n",
        "                  [0., 0., 0., 0.],\n",
        "                  [0., 0., 0., 0.]], dtype=float32), \n",
        "           array([[0., 0., 0., 0.],\n",
        "                  [0., 0., 0., 0.],\n",
        "                  [0., 0., 0., 0.],\n",
        "                  [0., 0., 0., 0.],\n",
        "                  [0., 0., 0., 0.]], dtype=float32)]\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik8vXZ1WEmLC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "            state_a = tf.split(state_a, self._num_blocks, axis=1)\n",
        "            assert len(state_a) == self._num_blocks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PDulYVcFc3n",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "state = [array([[1., 1., 1., 1.],\n",
        "                [1., 1., 1., 1.],\n",
        "                [1., 1., 1., 1.],\n",
        "                [1., 1., 1., 1.],\n",
        "                [1., 1., 1., 1.]], dtype=float32), \n",
        "         array([[2., 2., 2., 2.],\n",
        "                [2., 2., 2., 2.],\n",
        "                [2., 2., 2., 2.],\n",
        "                [2., 2., 2., 2.],\n",
        "                [2., 2., 2., 2.]], dtype=float32)]\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIU8Yk-n1S2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "            # Split the hidden state into blocks (each U, V, W are shared across blocks).\n",
        "            state = tf.split(state, self._num_blocks, axis=1)\n",
        "            assert len(state) == self._num_blocks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PJwqzilI2OK",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "j = 0\n",
        "state_j = [[1. 1. 1. 1.]\n",
        "           [1. 1. 1. 1.]\n",
        "           [1. 1. 1. 1.]\n",
        "           [1. 1. 1. 1.]\n",
        "           [1. 1. 1. 1.]]\n",
        "\n",
        "j = 1\n",
        "state_j = [[2. 2. 2. 2.]\n",
        "           [2. 2. 2. 2.]\n",
        "           [2. 2. 2. 2.]\n",
        "           [2. 2. 2. 2.]\n",
        "           [2. 2. 2. 2.]]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XzVcIjw1XGi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "            next_states = []\n",
        "            next_a_states = []\n",
        "            for j, state_j in enumerate(state): # Hidden State (j)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnzw7AzFK15-",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "keys = [[1., 1., 1., 1.], \n",
        "        [2., 2., 2., 2.]]\n",
        "\n",
        "j = 0\n",
        "key_j = [[1., 1., 1., 1.]]\n",
        "\n",
        "j = 1\n",
        "key_j = [[2., 2., 2., 2.]]\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38ZeZuXHJjcA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "                key_j = tf.expand_dims(self._keys[j], axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAZJm0CqMCdZ",
        "colab_type": "text"
      },
      "source": [
        "* get_candidate: Seccion 15\n",
        "\n",
        "```\n",
        "candidate_j = [[15.],\n",
        "               [15.],\n",
        "               [15.],\n",
        "               [15.]]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mODda6IjLlWG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "                candidate_j = self.get_candidate(state_j, key_j, inputs, U, V, W, U_bias)\n",
        "\n",
        "                reuse = False\n",
        "                if j != 0:\n",
        "                    reuse = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzYWTQ4SRzjo",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "num_units_per_block = embedding_size = 4\n",
        "\n",
        "w_ru = (4*2 x 4*2) = (8 x 8)\n",
        "b_ru = (4*2 x 1)   = (8 x 1)\n",
        "w_c  = (4*2 x 4)   = (8 x 4)\n",
        "b_c  = (4 x 1)     = (4 x 1)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lX0-N9rY135W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "                with tf.variable_scope(\"entnet_gru\", reuse=reuse) as gru_scope:\n",
        "                    w_ru = tf.get_variable(\n",
        "                        \"w_ru\", \n",
        "                        [self._num_units_per_block * 2, self._num_units_per_block * 2]\n",
        "                    )\n",
        "      \n",
        "                    b_ru = tf.get_variable(\n",
        "                        \"b_ru\", [self._num_units_per_block * 2],\n",
        "                        initializer=init_ops.constant_initializer(1.0))\n",
        "        \n",
        "                    w_c = tf.get_variable(\"w_c\",\n",
        "                        [self._num_units_per_block * 2, self._num_units_per_block]\n",
        "                    )\n",
        "          \n",
        "                    b_c = tf.get_variable(\n",
        "                        \"b_c\", [self._num_units_per_block],\n",
        "                        initializer=init_ops.constant_initializer(0.0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3L3LY2tU_pv",
        "colab_type": "text"
      },
      "source": [
        "* gru_block_cell: Celda GRU de tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkGHQRUk2Kma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "                    _gru_block_cell = gen_gru_ops.gru_block_cell  # pylint: disable=invalid-name\n",
        "  \n",
        "                    _, _, _, new_a = _gru_block_cell(\n",
        "                        x=candidate_j, h_prev=state_a[j], \n",
        "                        w_ru=w_ru, w_c=w_c, b_ru=b_ru, b_c=b_c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzdUQ_z4UbVX",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "num_units_per_block = embedding_size = 4\n",
        "v_a = (4 x 1)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwpGhy_oUaAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "                    v_a = tf.get_variable(\n",
        "                        \"v_a\", [self._num_units_per_block],\n",
        "                        initializer=self._initializer,\n",
        "                    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExFtkpSPVSny",
        "colab_type": "text"
      },
      "source": [
        "* get_gate: Seccion 14"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hlERtKE2qHO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "                next_a_states.append(new_a)\n",
        "\n",
        "                gate_j = self.get_gate(state_j, key_j, inputs, v_a, new_a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBaxP58152oZ",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://i.ibb.co/x8P2Lsr/liu2018-eq4.png\" alt=\"liu2018-eq4\" border=\"0\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peMM5vBz2s19",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "                # Equation 4: h_j <- h_j + g_j * h_j^~\n",
        "                # Perform an update of the hidden state (memory).\n",
        "                state_j_next = state_j + tf.expand_dims(gate_j, -1) * candidate_j"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUL6Fid32vmf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "                # Equation 5: h_j <- h_j / \\norm{h_j}\n",
        "                # Forget previous memories by normalization.\n",
        "                state_j_next_norm = tf.norm(\n",
        "                    tensor=state_j_next,\n",
        "                    ord='euclidean',\n",
        "                    axis=-1,\n",
        "                    keep_dims=True)\n",
        "                \n",
        "                state_j_next_norm = tf.where(\n",
        "                    tf.greater(state_j_next_norm, 0.0),\n",
        "                    state_j_next_norm,\n",
        "                    tf.ones_like(state_j_next_norm))\n",
        "                \n",
        "                state_j_next = state_j_next / state_j_next_norm\n",
        "\n",
        "                next_states.append(state_j_next)\n",
        "            \n",
        "            state_next = tf.concat(next_states, axis=1)\n",
        "            state_a_next = tf.concat(next_a_states, axis=1)\n",
        "            \n",
        "            return state_next, tf.concat(values=[state_next, state_a_next], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUdqB5kEaFS1",
        "colab_type": "text"
      },
      "source": [
        "# =================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGVErlNGaGXM",
        "colab_type": "text"
      },
      "source": [
        "# 17. Zero nil slot<br>\n",
        "\n",
        "Parametros:\n",
        "\n",
        "* t: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agMB0lJUaKJT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def zero_nil_slot(t, name=None):\n",
        "    \"\"\"\n",
        "    Overwrites the nil_slot (first row) of the input Tensor with zeros.\n",
        "\n",
        "    The nil_slot is a dummy slot and should not be trained and influence\n",
        "    the training algorithm.\n",
        "    \"\"\"\n",
        "    with name_scope(values=[t], name=name, default_name=\"zero_nil_slot\") as name:\n",
        "        t = tf.convert_to_tensor(t, name=\"t\")\n",
        "        s = tf.shape(t)[1]\n",
        "        z = tf.zeros(tf.stack([1, s]))\n",
        "        \n",
        "        return tf.concat(\n",
        "            axis=0, values=[z, tf.slice(t, [1, 0], [-1, -1])], name=name\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjTfTra_aNDy",
        "colab_type": "text"
      },
      "source": [
        "# 18. PReLU<br>\n",
        "\n",
        "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/cbc8zdv/prelu.png\" alt=\"prelu\" border=\"0\"></a>\n",
        "\n",
        "$a_{i}$ es un coeficiente que controla la pendiente de la parte negativa. Si $a_{i}=0$ es la funcion ReLU.\n",
        "\n",
        "Parametros:\n",
        "\n",
        "* features: vector de caracteristicas\n",
        "* alpha: coeficiente que controla la pendiente de la parte negativa\n",
        "* scope: ambito\n",
        "\n",
        "Funciones:\n",
        "\n",
        "* tf.nn.relu: funcion de activacion ReLU\n",
        "\n",
        "<img src=\"http://www.diegocalvo.es/wp-content/uploads/2018/12/funci%C3%B3n-ReLU-300x63.png\" alt=\"relu\" border=\"0\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04bOwhDTaNOz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prelu(features, alpha, scope=None):\n",
        "    \"\"\"\n",
        "    Implementation of [Parametric ReLU](https://arxiv.org/abs/1502.01852) borrowed from Keras.\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(scope, 'PReLU'):\n",
        "        pos = tf.nn.relu(features)\n",
        "        neg = alpha * (features - tf.abs(features)) * 0.5 \n",
        "        # (features - tf.abs(features)) * 0.5 = features (si features es negativo)\n",
        "        # (features - tf.abs(features)) * 0.5 = 0.0      (si features es positivo)\n",
        "        return pos + neg"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}