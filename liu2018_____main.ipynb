{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "liu2018_____main.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Em-BGdQ-4yK-",
        "colab_type": "text"
      },
      "source": [
        "# Preparacion del datset y parametros del modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMKjWOvMCErD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.flags.DEFINE_float(\"learning_rate\", 0.05, \"Learning rate for the optimizer.\")\n",
        "tf.flags.DEFINE_float(\"max_grad_norm\", 5.0, \"Clip gradients to this norm.\")\n",
        "tf.flags.DEFINE_integer(\"evaluation_interval\", 1, \"Evaluate and print results every x epochs\")\n",
        "tf.flags.DEFINE_integer(\"batch_size\", 128, \"Batch size for training.\")\n",
        "tf.flags.DEFINE_integer(\"epochs\", 800, \"Number of epochs to train for.\")\n",
        "tf.flags.DEFINE_integer(\"embedding_size\", 20, \"Embedding size for embedding matrices.\")\n",
        "tf.flags.DEFINE_integer(\"sentence_len\", 50, \"Maximum len of sentence.\")\n",
        "tf.flags.DEFINE_string(\"task\", \"Sentihood\", \"Sentihood\")\n",
        "tf.flags.DEFINE_integer(\"random_state\", 67, \"Random state.\")\n",
        "tf.flags.DEFINE_string(\"data_dir\", \"data/sentihood/\", \"Directory containing Sentihood data\")\n",
        "tf.flags.DEFINE_string(\"opt\", \"ftrl\", \"Optimizer [ftrl]\")\n",
        "tf.flags.DEFINE_string(\"embedding_file_path\", None, \"Embedding file path [None]\")\n",
        "tf.flags.DEFINE_boolean(\"update_embeddings\", False, \"Update embeddings [False]\")\n",
        "tf.flags.DEFINE_boolean(\"case_folding\", True, \"Case folding [True]\")\n",
        "tf.flags.DEFINE_integer(\"n_cpus\", 6, \"N CPUs [6]\")\n",
        "tf.flags.DEFINE_integer(\"n_keys\", 7, \"Number of keys [7]\")\n",
        "tf.flags.DEFINE_integer(\"n_tied\", 2, \"Number of tied keys [2]\")\n",
        "tf.flags.DEFINE_float(\"entnet_input_keep_prob\", 0.8, \"entnet input keep prob [0.8]\")\n",
        "tf.flags.DEFINE_float(\"entnet_output_keep_prob\", 1.0, \"entnet output keep prob [1.0]\")\n",
        "tf.flags.DEFINE_float(\"entnet_state_keep_prob\", 1.0, \"entnet state keep prob [1.0]\")\n",
        "tf.flags.DEFINE_float(\"final_layer_keep_prob\", 0.8, \"final layer keep prob [0.8]\")\n",
        "tf.flags.DEFINE_float(\"l2_final_layer\", 1e-3, \"Lambda L2 final layer [1e-3]\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zc1gJ6mQMr8X",
        "colab_type": "text"
      },
      "source": [
        "**logger** es usado para hacer seguimiento del debug<br> \n",
        "**assert** es para capturar excepciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SdOSfq2NbS-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logger = logging.getLogger()\n",
        "    logger.setLevel(logging.DEBUG)\n",
        "    ch = logging.StreamHandler()\n",
        "    ch.setLevel(logging.DEBUG)\n",
        "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "    ch.setFormatter(formatter)\n",
        "    logger.addHandler(ch)\n",
        "\n",
        "    logger.info(\" \".join(sys.argv))\n",
        "    logger.info(\"Started Task: %s\" % FLAGS.task)\n",
        "    \n",
        "    logger.info(pp.pformat(FLAGS.__flags))\n",
        "\n",
        "    session_conf = tf.ConfigProto(\n",
        "        intra_op_parallelism_threads=FLAGS.n_cpus,\n",
        "        inter_op_parallelism_threads=FLAGS.n_cpus,\n",
        "    )\n",
        "\n",
        "    aspect2idx = {\n",
        "        'general': 0,\n",
        "        'price': 1,\n",
        "        'transit-location': 2,\n",
        "        'safety': 3,\n",
        "    }\n",
        "\n",
        "    assert FLAGS.n_keys >= 2\n",
        "    assert FLAGS.n_tied == 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BA77rNYCA1I4",
        "colab_type": "text"
      },
      "source": [
        "**load_task** esta en **data_utils_sentihood.py**, se encarga de leer y tokenizar las reseñas en los archivos de entrenamiento y prueba. **FLAGS.data_dir** es la ubicacion de los archivos, **aspect2idx** es el id de los aspectos: <br>\n",
        "'general': 0, 'price': 1, 'transit-location': 2, 'safety': 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNBQhbSA_7Et",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    with tf.Session(config=session_conf) as sess:\n",
        "\n",
        "        np.random.seed(FLAGS.random_state)\n",
        "\n",
        "        # task data\n",
        "        (train, train_aspect_idx), (val, val_aspect_idx), (test, test_aspect_idx) = load_task(FLAGS.data_dir, aspect2idx)\n",
        "        \n",
        "        if FLAGS.case_folding:\n",
        "            train = lower_case(train)\n",
        "            val = lower_case(val)\n",
        "            test = lower_case(test)\n",
        "\n",
        "        data = train + val + test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwsSIa1CIz8Z",
        "colab_type": "text"
      },
      "source": [
        "Se obtiene el tamaño maximo de las oraciones, objetivos (entidades) y aspectos para establecer el tamaño de las cadenas de memorias y los embeddings de los objetivos (entidades) y aspectos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAkcbkrAIChi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "        max_sentence_len = max(map(lambda x: len(x[1]), data))\n",
        "        max_sentence_len = min(FLAGS.sentence_len, max_sentence_len)\n",
        "        logger.info('Max sentence len: %d' % max_sentence_len)\n",
        "        max_target_len = 1 # should be one\n",
        "        max_aspect_len = max(map(lambda x: len(x), [d[3] for d in data]))\n",
        "        assert max_aspect_len == 2\n",
        "        logger.info('Max target size: %d' % max_target_len)\n",
        "        logger.info('Max aspect size: %d' % max_aspect_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hFSzCuFJeGu",
        "colab_type": "text"
      },
      "source": [
        "Obteniendo embeddings "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyqSexQlJZj1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "        assert FLAGS.embedding_file_path is not None\n",
        "        word_vocab = EmbeddingVocabulary(\n",
        "            in_file=FLAGS.embedding_file_path,\n",
        "        )\n",
        "        word_vocab_processor = EmbeddingVocabularyProcessor(\n",
        "            max_document_length=max_sentence_len,\n",
        "            vocabulary=word_vocab,\n",
        "        )\n",
        "        embedding_mat = word_vocab.embeddings\n",
        "        embedding_size = word_vocab.embeddings.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHYvIFdoc0fg",
        "colab_type": "text"
      },
      "source": [
        "Generando IDs de las etiquetas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNdlCdSvc0uW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "        label_vocab = LabelVocabulary()\n",
        "        label_vocab_processor = LabelVocabularyProcessor(\n",
        "            vocabulary=label_vocab,\n",
        "            min_frequency=0,\n",
        "        )\n",
        "    \n",
        "        positive_idx = label_vocab.get('Positive')\n",
        "        negative_idx = label_vocab.get('Negative')\n",
        "        none_idx = label_vocab.get('None')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_L0iQxWUfTWe",
        "colab_type": "text"
      },
      "source": [
        "Transformando los datasets en vectores "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yArOuEtfTkc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "        train_sentences, train_targets, train_loc_indicators, train_aspects, train_labels, train_ids = vectorize_data(\n",
        "            train,\n",
        "            max_sentence_len,\n",
        "            max_target_len,\n",
        "            max_aspect_len,\n",
        "            word_vocab_processor,\n",
        "            label_vocab_processor,\n",
        "        )\n",
        "  \n",
        "        val_sentences, val_targets, val_loc_indicators, val_aspects, val_labels, val_ids = vectorize_data(\n",
        "            val,\n",
        "            max_sentence_len,\n",
        "            max_target_len,\n",
        "            max_aspect_len,\n",
        "            word_vocab_processor,\n",
        "            label_vocab_processor,\n",
        "        )\n",
        "        \n",
        "        test_sentences, test_targets, test_loc_indicators, test_aspects, test_labels, test_ids = vectorize_data(\n",
        "            test,\n",
        "            max_sentence_len,\n",
        "            max_target_len,\n",
        "            max_aspect_len,\n",
        "            word_vocab_processor,\n",
        "            label_vocab_processor,\n",
        "        )\n",
        "        \n",
        "        target_terms = [['location1'], ['location2']]\n",
        "        target_terms = word_vocab_processor.transform(target_terms)[:, :max_target_len]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_vLrSAojY4E",
        "colab_type": "text"
      },
      "source": [
        "Imprimiendo informacion de los dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qat_VjbfjZCT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "        sentence_len = max_sentence_len\n",
        "        vocab_size = len(word_vocab)\n",
        "        answer_size = len(label_vocab)\n",
        "  \n",
        "        logger.info(\"Training sentences shape \" + str(train_sentences.shape))\n",
        "        logger.info(\"Training targets shape \" + str(train_targets.shape))\n",
        "        logger.info(\"Training aspects shape \" + str(train_aspects.shape))\n",
        "        logger.info(\"Validation sentences shape \" + str(val_sentences.shape))\n",
        "        logger.info(\"Validation targets shape \" + str(val_targets.shape))\n",
        "        logger.info(\"Validation aspects shape \" + str(val_aspects.shape))\n",
        "        logger.info(\"Test sentences shape \" + str(test_sentences.shape))\n",
        "        logger.info(\"Test targets shape \" + str(test_targets.shape))\n",
        "        logger.info(\"Test aspects shape \" + str(test_aspects.shape))\n",
        "        \n",
        "        # params\n",
        "        n_train = train_sentences.shape[0]\n",
        "        n_val = val_sentences.shape[0]\n",
        "        n_test = test_sentences.shape[0]\n",
        "        \n",
        "        logger.info(\"Training Size %d\" % n_train)\n",
        "        logger.info(\"Validation Size %d\" % n_val)\n",
        "        logger.info(\"Testing Size %d\" % n_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBpHR-KJwcLb",
        "colab_type": "text"
      },
      "source": [
        "Dividiendo el dataset de entrenamiento por su categoria (positivo, negativo, ninguno)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwcvoj2Jwcdc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "        train_positive_idx = np.where(train_labels == positive_idx)[0]\n",
        "        train_negative_idx = np.where(train_labels == negative_idx)[0]\n",
        "        train_none_idx = np.where(train_labels == none_idx)[0]\n",
        "\n",
        "        train_positive_sentences = train_sentences[train_positive_idx]\n",
        "        train_positive_targets = train_targets[train_positive_idx]\n",
        "        train_positive_aspects = train_aspects[train_positive_idx]\n",
        "        train_positive_labels = train_labels[train_positive_idx]\n",
        "\n",
        "        train_negative_sentences = train_sentences[train_negative_idx]\n",
        "        train_negative_targets = train_targets[train_negative_idx]\n",
        "        train_negative_aspects = train_aspects[train_negative_idx]\n",
        "        train_negative_labels = train_labels[train_negative_idx]\n",
        "\n",
        "        train_none_sentences = train_sentences[train_none_idx]\n",
        "        train_none_targets = train_targets[train_none_idx]\n",
        "        train_none_aspects = train_aspects[train_none_idx]\n",
        "        train_none_labels = train_labels[train_none_idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9p3qNthyBOE",
        "colab_type": "text"
      },
      "source": [
        "Asegurandose que los id de las categorias no sean iguales"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B320H8nYyBXf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "        assert len(train_none_idx) > len(train_positive_idx)\n",
        "        assert len(train_positive_idx) > len(train_negative_idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo1WqWCz2mIJ",
        "colab_type": "text"
      },
      "source": [
        "Tamaños de los grupos formados al dividir el dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6lPvmPd2mST",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "        n_positive_train = len(train_positive_idx)\n",
        "        n_negative_train = len(train_negative_idx)\n",
        "        n_none_train = len(train_none_idx)\n",
        "        n_train = n_negative_train # down-sampling\n",
        "\n",
        "        logger.info(\"Positive training Size %d\" % n_positive_train)\n",
        "        logger.info(\"Negative training Size %d\" % n_negative_train)\n",
        "        logger.info(\"None training Size %d\" % n_none_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTEuNxDP2_No",
        "colab_type": "text"
      },
      "source": [
        "Eligiendo un optimizador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEGcH0Dv2_YE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "        if FLAGS.opt == 'adam':\n",
        "            optimizer = tf.train.AdamOptimizer(\n",
        "                learning_rate=FLAGS.learning_rate, epsilon=FLAGS.epsilon)\n",
        "        elif FLAGS.opt == 'ftrl':\n",
        "            optimizer = tf.train.FtrlOptimizer(\n",
        "                learning_rate=FLAGS.learning_rate\n",
        "            )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41e9T-EW4AYq",
        "colab_type": "text"
      },
      "source": [
        "Creando el modelo<br>\n",
        "**answer_size** es el tamaño de la salida, un vercor de tamaño 3 (positivo, negativo, ninguno)<br>\n",
        "**embedding_mat**  es el vocabulario de embeddings<br>\n",
        "**tied_keys** es el vocavulario de los objetivos (entidades)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcsjHMwJ4Aos",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "        model = Delayed_EntNet_Sentihood(\n",
        "            batch_size, \n",
        "            vocab_size, \n",
        "            max_target_len,\n",
        "            max_aspect_len,\n",
        "            sentence_len, \n",
        "            answer_size,\n",
        "            embedding_size, \n",
        "            session=sess,\n",
        "            embedding_mat=word_vocab.embeddings,\n",
        "            update_embeddings=FLAGS.update_embeddings,\n",
        "            n_keys=FLAGS.n_keys,\n",
        "            tied_keys=target_terms,\n",
        "            l2_final_layer=FLAGS.l2_final_layer,\n",
        "            max_grad_norm=FLAGS.max_grad_norm, \n",
        "            optimizer=optimizer,\n",
        "            global_step=global_step\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9rlDBs74Mxx",
        "colab_type": "text"
      },
      "source": [
        "Entrenando el modelo para cada categoria del dataset de entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2qcNirU4M9b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "            for start, end in batches:\n",
        "                # train negative\n",
        "                sentences = train_negative_sentences[start:end]\n",
        "                targets = train_negative_targets[start:end]\n",
        "                aspects = train_negative_aspects[start:end]\n",
        "                answers = train_negative_labels[start:end]\n",
        "                cost_t = model.fit(sentences, targets, aspects, answers,\n",
        "                                   FLAGS.entnet_input_keep_prob,\n",
        "                                   FLAGS.entnet_output_keep_prob,\n",
        "                                   FLAGS.entnet_state_keep_prob,\n",
        "                                   FLAGS.final_layer_keep_prob)\n",
        "                total_cost += cost_t\n",
        "                total_training_instances += len(train_negative_sentences[start:end])\n",
        "\n",
        "                # train positive\n",
        "                positive_start = random.randint(0, n_positive_train - batch_size)\n",
        "                positive_end = positive_start + batch_size\n",
        "                sentences = train_positive_sentences[positive_start:positive_end]\n",
        "                targets = train_positive_targets[positive_start:positive_end]\n",
        "                aspects = train_positive_aspects[positive_start:positive_end]\n",
        "                answers = train_positive_labels[positive_start:positive_end]\n",
        "                cost_t = model.fit(sentences, targets, aspects, answers, \n",
        "                                   FLAGS.entnet_input_keep_prob,\n",
        "                                   FLAGS.entnet_output_keep_prob,\n",
        "                                   FLAGS.entnet_state_keep_prob,\n",
        "                                   FLAGS.final_layer_keep_prob)\n",
        "                total_cost += cost_t\n",
        "                total_training_instances += len(train_positive_sentences[positive_start:positive_end])\n",
        "\n",
        "                # train none\n",
        "                none_start = random.randint(0, n_none_train - batch_size)\n",
        "                none_end = none_start + batch_size\n",
        "                sentences = train_none_sentences[none_start:none_end]\n",
        "                targets = train_none_targets[none_start:none_end]\n",
        "                aspects = train_none_aspects[none_start:none_end]\n",
        "                answers = train_none_labels[none_start:none_end]\n",
        "                cost_t = model.fit(sentences, targets, aspects, answers, \n",
        "                                   FLAGS.entnet_input_keep_prob,\n",
        "                                   FLAGS.entnet_output_keep_prob,\n",
        "                                   FLAGS.entnet_state_keep_prob,\n",
        "                                   FLAGS.final_layer_keep_prob)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}